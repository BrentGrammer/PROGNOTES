# S3 Simple Storage

### **PRIVATE BY DEFAULT** - permissions are locked down and you have to allow access and policies to use S3.

- Only the account root user has access to a bucket by default.

- inifitely scalable storage (Buckets can store unlimited amount of data)
- Public service (runs in public AWS Zone)
- Region based (possible to replicate across regions)
- Object storage system (not a file or block storage system)
  - Good for accessing the whole object (images, audio files, etc.)
  - Not block storage that you can mount as a drive into images etc.
  - Good for offloading data and storing it
  - usually used for input and output to AWS services (most services offer integration with S3)

## Use Cases

- backup and storage
- Disaster Recovery
- Archiving
- Hybrid Cloud Storage (extend from on premise)
- Application hosting
- Media hosting
- Data lakes and big data analytics
- Software delivery
- Static website hosting

### Objects: files stored in S3

- Two parts: a key and a value (key is like a file name, the value is the data)
- Have a key which is the full path: s3://my-bucket/myfile.txt
- key is composed of the prefix (directories with slashes) and the file name
- **MAX SIZE**: From 0 bytes to 5 TB
  - must use a multi-part upload to upload larger files
- Have Metadat and tags that can be referenced and a version ID
- Objects are not actual files - they are just objects with a key.

## Buckets:

- like directories for the files
- Must have **globally unique name** across all regions and all accounts
  - see [rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html?icmpid=docs_amazons3_console)
  - Must be between 3 and 63 chars
  - Must start with lowercase letter or number
    - CANNOT begin with `X` or `N`
  - can contain dots and dashes
  - bucket names can't be IP formatted i.e. 1.1.1.1
  - soft limit of 100 buckets in an account (not per region), hard limit of 1000 (using support requests) - remember this to structure a system with many users - use prefixes with one bucket instead of a bucket per user for example.
- Tied to a specific region! Data has a primary home region and does not leave that unless configured to.
- Has to be defined at the region level and assigned a region
- there is naming convention restrictions
- Blast Radius is region level. (large scale failures are confined to the region)
- Note: all objects are stored at the root level of a bucket (there is no file directory heirarchy)
  - you can still have keys that have prefixes in the key that can simulate nested folders

#### Security Issue: Bucket names

- You are charged for bad requests or denied requests so make sure your bucket name cannot be guessed and is sufficiently unique and hard to guess.

### Cleaning/Deleting up buckets

- two steps:
  - you need to empty the bucket (select the bucket on the s3 page and click `empty` in the console)
  - and then click `delete`.

## Permissions for S3 Buckets

### S3 Bucket Policy

- a type of **resource policy** (a policy that is attached to a resource instead of an identity)
  - Controls who can access that resource
  - Unlike identity policies, resource policies can allow/deny identities from the same account or a different account. \*Ability to grant other identities from other accounts permissions\*\*
  - Can also allow Anonymous Principals (grants access to the world)
- Resource policies have a `Principal` property that defines which identities/principals are affected by the policy (who is impacted)
  - Identity policies do not have this as the principal for them is the identity
- You can only have ONE bucket policy on a Bucket, but it can have multiple statements.

### When to use Identity vs. Resource Policy

- If you need to grant permissions to many different resources across an AWS account, then use Identity policies (not every resource has resource policies and you'd need to create a resource policy for every service - with an identity policy you can have one policy that specifies permissions for all the different resources)
- Identity management in one place means you want to use IAM (resource policies do not have a central place for management)
- Resource policies make sense for allowing anonymous outside users access to a service.

### Access

- For any identity in an AWS account is accessing a bucket inside that same account, then access is a combination of all the applicable identity policies for the user and the resource bucket policy.
- For anonymous access, only the bucket policy applies (no identity policies since they are not logged in)
- For users from another AWS Account,
  - they need an identity policy to allow access to your bucket,
  - and you need to allow them in the bucket policy (2 step process)

### Implementing Bucket Policies

- You can specify IP addresses that are allowed, for example:

```json
{
  "Version": "2012-10-17",
  "Id": "BlockUnLeet",
  "Statement": [
    {
      "Sid": "IPAllow",
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:*",
      "Resource": "arn:aws:s3:::secretproject/*", // copy the arn on the edit bucket policy page and make sure to leave the `/*` at the end after it to allow access to all objects
      "Condition": {
        "NotIpAddress": { "aws:SourceIp": "1.3.3.7/32" } // deny everyone except this IP (this statement does not apply and this IP gets any other permissions allowed.)
      }
    }
  ]
}
```

- Can also specify multi factor auth is required to use the bucket, etc.

## Pricing

- https://aws.amazon.com/s3/pricing/
- Per GB per month charge (very cheap)
- Transfer charge for data in and out per GB (very cheap)
- Charged for Operations such as GET, PUT, DELETE etc.
  - charged per 1000 requests
  - Be careful if you have a lot of users or operations - charges add up.

# S3 Security

- User Based
  - IAM Policies - which API calls are allowed for a specific IAM user
  - Use case - attach a IAM policy to a user to access a bucket (no bucket policy needed)
  - For EC2 Instances use a role that has access policy for S3 buckets attached to it
- Resource Based
  - Bucket Policies allow cross account access
  - Use case is Public Access - attach a bucket policy that allows public access
  - Use case is a user that is on another AWS account that needs access - use a bucket
    policy
  - can also use them to force encryption
- IAM principal can access an S3 Object if either the user IAM policy or bucket policy allows it.
  - \*There is no explicit DENY policy
- Encryption - use encryption keys for bucket contents

### ACLs (Access Control Lists)

- Less commonly used currently and are considered legacy vs. bucket policies
- not as flexible as bucket policies and can only have simple permissions
- Can be applied to subresources such as objects in buckets.
- ACLs cannot affect a group of objects which is a major drawback from bucket policies which allow that flexibility.

## Public Access

- When creating a bucket if you uncheck **Block all** access it does NOT make the bucket publically accessible, but will allow you to configure it for public access if desired. It is a fail safe that will override any configuration you make to make the bucket public.

  - Block Public Access policies were implemented for security and apply regardless of the policies created (they apply only to anyonymous principals)
  - If you have granted public access via a bucket policy and it still doesn't work, these settings are probably set and you need to adjust them to allow access.

- All objects and buckets are private by default (you need to authenticate first) unless configured otherwise.
  - Example if you open an object link from the console in a new tab you will get Access Denied because you're accessing it as an unauthenticated user in a new fresh tab.
  - You need to use the `Open` button in AWS S3 Console to open the file in the browser which will authenticate you.

### Setting up policy for public Access

- You need to uncheck all the block public access settings first
- Can use the policy generator in the GUI to generate a policy
- The ARN should end with a slash and the bucket directories or files
  - `<arn>/*`
- Useful for allowing accesses to websites hosted on S3

### Static Website Hosting

- Enable the static website option in properties for the S3 settings
- make sure block settings are disabled and policy for public access is setup.

# Object Versioning

- [video explanation](https://learn.cantrill.io/courses/1101194/lectures/25870664)
- [Video demo implementing versioning](https://learn.cantrill.io/courses/1101194/lectures/25871102)
- Can enable file versioning at the bucket level
- Any operations that modify an object with versioning enabled will generate a new object and leave the original in place.
- To see versions in AWS console select the `Show versions` toggle on the specific bucket page [in AWS console](https://learn.cantrill.io/courses/1101194/lectures/25871102) (see timetamp 6:12)

### Suspending Bucket Versioning

- **IMPORTANT**: Once you enable versioning, you cannot disable it and go back to non-versioned objects!
- Versioning can be suspended on a bucket as an alternative and re-enabled later if needed.
- AWS Console > S3 > Click bucket > Properties Tab > Versioning section - click Edit, select Suspend

### Bucket Ids

- With versioning off, the `id` attribute on a object in a bucket will be `null`
- Turning Versioning on will allocate an `id` value to each object
- Same key (filename) overwrite will update the version with a new `id` and preserve the original object with it's `id`

### Current Version

- The newest version of an object in a versioned enabled bucket is called the `Current Version`
  - If a version is not specified in the request, then the current version will always be returned

### Deleting versioned Objects (Delete Markers)

- [video demo of delete markers](https://learn.cantrill.io/courses/1101194/lectures/25871102) at timestamp 9:58
- IF no version is specified in delete request, S3 will create a new special version of the obj known as a "Delete Marker"

  - The object isn't actually deleted, but the delete marker will hide all previous versions of that object

  #### Delete Markers

- Deleting the "Delete Marker" will return the last current version of the object to be active again.
  - **Toggle off `show versions` on the bucket page, select the shown current version of the object and then click `Delete` to add a delete marker**
  - equivalent to an undelete for the object

#### Permanently Deleting Objects

- To Delete an object totally, you need to specify the version. The next most recent version if it exists will become the new current version.
- **Toggle on `show versions` and then select the version to delete, then click `Delete`**
- This makes the next most recent version of the object the current version.

### Storage considerations with versioning

- All objects remain and will incur storage costs and space
- The only way to zero out costs is to delete the bucket and re-upload all the files
  - Suspending does NOT cancel the storage costs.
- Best practice is to version buckets
  - restore from unintended delete and rollback file versions. To restore a file note that you delete the delete marker for it
- Note: files not versioned before enabling it will have a version of "null"
  - suspending versioning does not delete previous versions
- Setup: Enable bucket versioning in the properties tab of aws console gui for s3 bucket. This will auto generate versions of files uploaded for that bucket

## MFA Delete

- You can optionally enable this in the versioning configuration
- Multi Factor Auth will be required to change bucket versioning state
  - includes switching to suspended versioning, or deleting fully any versions of an object
- Requires a serial number plus code generated and pass that with API calls to delete or change version states.

# Access Logs

- for audit purposes or data analytics
- Logs are enabled and then written to another S3 bucket
  - NOTE: you need to create a new bucket for logging!
  - In the console you need to find menyu to enable Server access logging
    - For target bucket, remember to add `s3://` before the bucket name. You can also add a `/logs` to the end of the path
- Can take an hour or two before logs start populating after enabling

# S3 Replication

- CRR - Cross Region Replication
  - Use cases: compliance, lower latency access for distant/dispersed users, replication across accounts
- SRR - Same Region Replication
  - Use Cases: log aggregation, live replication between prod and test accounts
- Replication is Asynchronous in the background
- **Must enable (object/bucket) versioning in source and destination/target buckets**
- Bucket replicas can be in different accounts
- Must enable IAM permissions so buckets can copy over files
- Setup: Go to source bucket in AWS console and to Management tab, then create a replication Rule
  - note that version ids are replicated as well for files

# S3 Storage

- Types
  - Amazon S3 Standard - General Purpose
    - 99.99% Availability
    - Used for frequently accessed data
    - Low latency high throughput
    - can sustain 2 concurrent facility failures
    - Use Cases: Big Data Analytics, mobile and gaming apps, content distribution
  - Standard-Infrequent Access (IA)
  - One Zone-Infrequent Access
    - less frequently accessed files but requres rapid access when needed
    - lower cost than Standard
    - S3 Standard IA: 99.9% available (less than standard), used for disaster recovery and backups
    - S3 One Zone IA: 99.5% available, used to store backup copieds of data you can recreate or on prem data. \*Data is lost if an AZ is destroyed
  - Glacier: Low cost cold storage - pay for storage and retrieval cost
    - Glacier Instant Retrieval: millisecond retrieval - good for data accessed once a quarter. Min storage duration of 90 days, good for backup with fast retrieval
    - Glacier Flexible Retrieval
      - Expedited - 1 to 5 minutes retrieval
      - Standard - 3-5 hours for retrieval
      - Bulk - 5-12 hours for retrieval. This is free cost retrieval
      - Min. storage is 90 days
    - Glacier Deep Archive: long term storage
      - standard is 12 hours retrival, bulk is 48 hour wait for retreival
      - min. storage duration is 180 days
  - Intelligent Tiering
    - Small monthly monitoring and auto-tiering fees
    - No retrieval charges
    - Moves objects autopmatically between access tiers based on usage
    - Tiers:
      - Frequent Access: default tier
      - Infrequent Access: objects not accessed for 30 days
      - Archive Instant Access: not accessed for 90 days
      - Archive access(optional tier): configurable from 90-700 days
      - Deep Archive access(optional tier): configurable from 180-700 days
- Can move between classes manually or with lifecycles

S3 Durability: 11 9's - 99.999999999% - with 10,000,000 objects a loss can incur once every 10,000 years. This is for all storage classes in S3

Availability:

- Depends on storage class

- Can create Lifecycle Rules to determine when and where to move objects between storage class tiers based on your settings.

# LOCKS

- S3 Object Lock
  - WORM - write once read many - blocks object from editing or deletion for a time
- Glacier Vault Lock
  - Locks policy from editing for a time
- Used for compliance or scenarios where you need to retain a file unmodified for some time. Not even admins can edit it.

# S3 Encryption

- No Encryption
- Server Side - encrypted on backend after upload
- Client Side - encrypts before uploading

Plaintext: unencrypted data
Ciphertext: encrypted data

# AWS Storage Gateway

- Hybrid cloud solution
- Bridge on prem to S3 on the cloud
- Types: Tape Gateway, Amazon S3 File Gateway, Amazon FSx file Gateway, and Volume Gateway

# Performance and Reliability Optimizations

## Multipart Upload

- Uploading by default in S3 is a single stream of data: Data > (s3:PutObject) > Bucket
  - Note: Single uploads are limited to 5 GB (though practially, you would never rely on one stream for that size of data)
- If Uploading fails, the whole process has to be restarted from the beginning (any data that did make it is lost and you have to start the upload from the beginning)
- This limits the speed and reliability (if connections are unreliable)
- Even on fast connections, downloads/uploads usually should occur on multiple streams

### Multipart upload optimization

- \*\*Recommended to use this as soon as you can
- Breaks apart data into pieces to upload
- The minimum size required for using this is 100MB of data
- Uploads can be split into a max of 1000 parts
  - between sizes of 5MB to 5GB (the last part can be smaller than 5MB)

#### Main Advantage of Multipart Upload

- If pieces of data fail to transfer, they can be restarted individually rather than having to restart the entire data transfer from the beginning as you would in a single stream.
- The transfer rate (sum of parts) is faster using mutlipart upload by leveraging parallelism

## Accelerated Transfer

- [video](https://learn.cantrill.io/courses/1101194/lectures/25871120) at 9:00
- Data transferred is sent over the public internet via a path that we have no control over, and it can be inefficient sometimes.
- S3 Accelerated Transfer uses the AWS Global network of Edge locations instead of the public internet to send data through more efficient paths

### Enabling S3 Transfer Acceleration

- See [video](https://learn.cantrill.io/courses/1101194/lectures/25996479)
- [AWS Transfer Accerlation Tool](http://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html)
  - This tool shows you a comparison of upload speeds from where you are to each of the AWS regions with and without Acceleration Transfer on.
- Go to AWS Console > S3 > Click on bucket > Properties Tab > scroll down to find Transfer acceleration

  - Enabling it creates a new endpoint for the bucket that you need to use for the feature (ex: `bucketname.s3-accelerate.amazonaws.com`) - resolves to a edge location based on where you are in the world.

- **By default Accelerated Transfer is switched OFF in S3** - you need to enable it to take advantage of this feature.
- **RESTRICTIONS**
  - Bucket name cannot have any periods
  - Bucket name needs to be DNS compatible
- Note: the data goes through the public internet initially to get to the Edge, but they are really close geographically and positioned well.

### Main Advantage of Acceration Transfer:

- The advantages of using S3 Transfer Acceleration increase as the distance you're transferring data over increases.
- Especially good for transferring data over longer geographic distances

## S3 Server Side Encryption

- NOTE: Buckets are NOT encrypted, only Objects are (defined at the object level)
  - each object could use different encryption settings

### Server Side Encryption vs. Client Side Encryption

- [video](https://learn.cantrill.io/courses/1101194/lectures/25997330)
- defines how data is **encrypted at rest** (how data is stored on disc in an encryted way)
- Both methods use encryption in transit as well between the user and the s3 endpoint they call and send/receive data to/from

#### Client Side Encryption

- Client Side: encrypted by client before they are sent
  - data is ciphertext or encrypted the entire journey path from user to s3 endpoint to disc storage in AWS S3
    - https encrypts it in transit as well
  - More control, you're responsible for generating and keeping track of which keys are used for which object, you use your cpu power to do this and S3 is just used for storage and not involved in encryption in any way
  - used for high security situations with low trust in AWS/S3

#### Server Side Encryption

- Server Side: Data is not encrypted before sent (but is still sent through an https tunnel to the endpoint)
  - once data hits s3 endpoint it is encrypted by the servers and then goes to disc encrypted
  - allows S3 to handle some of the encryption process (requires trust in s3/AWS)
- Note: encryption at rest is required for use with S3 (storing encrypted at rest)

## Types of Server Side Encryption

- [Overview of types](https://learn.cantrill.io/courses/1101194/lectures/25997330) at timestamp 19:45
- Documentation
  https://docs.aws.amazon.com/AmazonS3/latest/user-guide/default-bucket-encryption.html

  https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html

  https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html

  https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html

  https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html

### SSE-C: with Customer Keys

- Customer is responsible for the keys, and S3 manages the encryption/decryption processes
  - Difference between this and Client side encryption is that S3 is handling the cryptographic operations
  - You need to trust AWS S3 to discard the encryption key
- You provide the unencrypted/plaintext data and the encryption key for encrypting it in client request
  - The plaintext data is encrypted as usual with https on it's way to S3
  - When at s3, the object is encrypted and hash of the key is tagged to the object and the key is destroyed.
    - The hash is **one way** and cannot be used to generate another key
    - You need to provide the same key to use for decryption and the hash can determine whether that was the key originally used to encrypt it or not.
      - if correct, s3 decrypts and returns the plaintext over an https tunnel(discarding the key sent again)
- Useful for saving cpu power on client if you have a lot of objects to encrypt/decrypt
- Also useful in regulation heavy environments to control and manage the keys
  - Use **Client Side encryption** when you need to manage both the keys and the encryption and decryption processes (this is needed when you don't trust AWS to see your plaintext data)

### SSE-S3

- Useful in most situations and lowers admin overhead
  - Not suitable in high regulation environment where you need control over the keys or you need to control the key rotation
  - **Does not allow role separation**: A system admin user, for example, can read encrypted data when using this approach and you cannot prevent that (not good in financial and medical industries for example)
- **uses AES-256 algorithm** - if you see this on the exam, think SSE-S3
- AWS manages both the encryption with AWS S3 Managed keys (default option), and the key management
- You only provide the plaintext data in the request to S3 (no keys)
- When data is uploaded to S3, it is encrypted with a key that is unique to every object
  - S3 generates a key just for that object which it uses to encrypt it
- **S3 KEY**: S3 creates a high level key that uses and encrypts the separately generated per object key to encrypt the object, and then the generated per object key is discarded
  - You have no control over options or anything about the key, and it is outside of your visibility or available in the AWS console, etc - it is rotated internally by S3
  - This leaves you with a ciphertext object and a ciphertext key which are stored on disc

### SSE-KMS

- Keys are managed with a separate service, KMS (instead of S3)
- Useful in regulated environments since you have control over the KMS key and it's rotation
  - **Allows for role separation** - to decrypt an object a user needs access to the KMS key originally used. If they don't have access permissions to KMS, they can't decrypt the stored encrypted DEK, and so they can't decrypt any objects
    - A full admin user for S3 has full control over a bucket in S3, but because they don't have access to KMS and the key, they can't decrypt any data in the bucket.
    - This is a big difference between SSE-KMS and SSE-S3
- Advantage comes from creating a customer managed KMS key
  - This key is created by you, managed by you, and has isolated permissions
  - the key is also fully configurable
- encryption with kms keys stored in key management service
- When an object needs to be encrypted, S3 needs to contact KMS and generate a new encryption key for the object using the key in KMS
  - KMS will give S3 two versions of the same Data Encryption Key (DEK), a plaintext version and a ciphertext(encrypted) version.
    - AWS S3 takes the plaintext DEK and the plaintext object and creates an encrypted version of the object, the plaintext key is immediately discarded, leaving only the ciphertext version of that key, and both the encrypted object and the encryupted key are stored on disc in S3 Storage.
    - Uses the same overall architecture as SSE-S3 (per object encryption key made with a higher level key), but the higher level key and DEK is created using a key managed by you and KMS instead of a key created or managed by S3.
    - The KMS key is used to decrypt the encrypted DEK stored with the object when you want to decrypt the object
- NOTE: KMS keys can only encrypt objects up to 4KB in size, so the KMS key is used to generate DEKs (Data Encryption Keys) which don't have that limitation
  - **KMS Does not store the DEKs, it only generates them and gives them to S3**
    - But, you still have control over the KMS key

### Setting up Encryption in S3

- [Demo Video](https://learn.cantrill.io/courses/1101194/lectures/25997332)
- [Choosing Encryption Type on Objects (when uploading)](https://learn.cantrill.io/courses/1101194/lectures/25997332https://learn.cantrill.io/courses/1101194/lectures/25997332) at timestamp 3:11
  - Shows setting SSE-S3 or SSE-KMS types of encryption on objects when uploading them.
    - Note: The default AWS KMS key will be shown as `arn:aws:kms:us-east-1:{acctnum}:alias/aws/s3`
    - Role separation demo shows how you can add a DENY policy to the iamadmin General account user to deny all actions for KMS
      - This will prevent a user from accessing objects that are encrypted with SSE-KMS, but **NOT** block them from the objects encrypted with SSE-S3 encryption (they still have permissions to do everything in S3, which does not go to KMS to get a key with SSE-S3)
  - [Shows where AWS managed KMS keys are for SSE-KMS if you don't select your own key](https://learn.cantrill.io/courses/1101194/lectures/25997332) at timestamp 10:55
    - Note how you cannot set any rotation or update policy for it, since it is managed and set by AWS.

### Setting Default Bucket Encryption

- [Shows how to specify default encryption to use for the bucket](https://learn.cantrill.io/courses/1101194/lectures/25997332) at timestamp 12:05
- Go to S3 > Select Bucket > Properties > Default Encrytion Section > Edit button
  - NOTE: this is only for setting the default method of encryption if the upload does not specify an encryption method (if it does, then it will use that over what is set here).

## S3 Bucket Keys - solve scaling issues with SSE-KMS

- A way to scale and reduce cost when using KMS Encryption
- Each object PUT operation using SSE-KMS uses a unique Data Encryption Key (DEK)

  - The operation uses the KMS key to generate a DEK unique to the Object
    - This is a unique API call to KMS to get that DEK for each object uploaded.

- The generation of DEKs per object upload creates scaling, cost and throttling issues
  - Throttling means there are limits to how many DEKs can be produced for large object uploads and has levels at 5,500, 10,000, or 50,000 per second (shared across regions)
  - Bucket Keys address these problems!

### Solving Scaling problems with Bucket Keys

- AWS KMS will generate a time limited Bucket Key given to the bucket which is used to generate DEKs for objects within S3.
  - Instead of having to make a call to KMS for every object upload
  - Offloads the work from KMS to S3
- NOTES on using Bucket Keys:
  - If using Cloudtrail to look at KMS logs, thos logs will show the bucket ARN instead of the object ARN.
  - You will see fewer Cloudtrail events for KMS since you're offloading work to S3 instead of KMS (KMS is just generating the bucket key initially and that's it for that limited time)
  - Bucket keys work with same region replication and cross region replication (replication maintains the same encryption settings)
  - If replicating plaintext objects to a destination that uses encryption, the ETAG will change
  - See [nuanced features documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-key.html)

# S3 Storage Classes

https://aws.amazon.com/s3/pricing/
https://aws.amazon.com/s3/storage-classes/

## S3 Standard - default

**Should be used for frequently accessed data that is important and non-replaceable**

- You should generally use this and only choose other storage classes if you have a specific reason for it.

### S3 Standard features

- **Stores data across at least 3 Availability Zones**
- Replication is 11 9s of durability
- Corruption detection and automatic repair on objects is done with Content-MD5 Checksums and Cycle Redundancy Checks (**CRCs**)
- When objects are finished being stored **durably** in S3, a Response is made that is HTTP 1.1/200 status
- millisends first byte latency (data is gotten in milliseconds)
- data can be made publicly available (via permissions or static public site hosting)

### S3 Billing

- Billed GigaByte per month charge for data transferred OUT
- Data tranferred IN is free
- Price also charged per 1,000 requests
- Note: **there is no specific retrieval fee, minimum duration or minimum size requirements/cost**
  - This is different from other storage classes
- Considered the most balanced option cost-wise

## S3 Standard-IA (Infrequent Access)

**Should be used for important long-lived data that is irreplaceable, but is not accessed often**

- Avoid using it for lots of small files or for temporary data

### Features

- data replicated over 3 AZs
- Like s3 standard as well, objects can be made publicly available, first byte latency is millisecs
- 11 9s of durability like S3 standard
- Good option if you don't need to store data short term and you don't have lots of tiny objects (due to min size charge see below)

### Billing for IA

- Billing is the same basic cost model as S3 standard, but **much cheaper for storing objects**
- Compomises for cheap cost:
  - Retreival Fee: cost to retreive an object in addition to the transfer OUT fee.
    - This means that the cost benefit is reduced if you access the data more frequently
  - Minimum duration charge: billed for a minimum of 30 days
  - Minimum size charge: billed for min. of 128KB per object

## S3 One Zone-IA (Infrequent Access)

**Use for long lived data, infrequently accessed and is non-critical which can be easily replaced**

- For example, can use it for replica copies (with same or cross region replication)
- Generating intermediate data that you can afford to lose.

- Cheaper than S3 Standard-IA
- Main difference is that data is only stored in one Availability Zone in the region (no replication)
- Higher risk to data loss if AZ fails
- 11 9s of durability assuming AZ does not fail
  - Data is still replicated inside the AZ

### Billing

- Retreival fee
- min 30 day charge
- min 128KB size per obj charge

## S3 Glacier Instant Retrieval

- Similar to infrequent access, but offers cheaper storage, more expensive retrieval costs, and longer minimums
- Designed to get data instantly, but not often (i.e. once a month or once every quarter)
- Minimum storage duration charge of 90 days (vs. 30 compared to Standard IA)
  - As object access decreases over time, you can move them from Standard, to Standard Infrequent Access and then to Glacier Instant Retrieval
- Overall it costs more if you need to access the data compared to other classes, but is cheaper if you don't

## S3 Glacier Flexible Retrieval (formerly S3 Glacier)

Used to store archival data where frequentor real-time access is not needed (i.e. yearly access, for ex.)

- Replication over 3 Availability Zones (same as S3 Standard and S3 Standard Infrequent Access)
- Same durability as them - 11 9s
- Storage cost is about 1/6th of S3 Standard Cost
- **Objects should be thought of as cold** - they are not warm and ready for use or immediately available and cannot be made public
- There are pointers to the objects
- You need to perform a retrieval process to get access to them (a job that needs to be run)
  - Retrieval process incurs cost, when you retrieve them they are stored in S3 Standard Infrequent Access class temporarily - when you access them they are removed.
    - You can retrieve them permanently by changing the class back to the S3 ones.
- First byte latency of minutes or hours (depending on job type below to retrieve)

### Retrieval Jobs (3 types)

- Expedited: data becomes available in one to five minutes (most expensive)
- Standard: data is accessible in 3-5 hours
- Bulk: data is available between 5 and 12 hours (cheapest)

### Limits

- 40kb minimum billable size

## S3 Glacier Deep Archive

Used for archival data storage that rarely if ever needs to be accessed. (not suited to system backups due to long restore time, more useful for data that is required to be kept for legal retention requirements.)

- The cheapest class of storage (much cheaper than the other classes)
- Data is in a "Frozen" state as opposed to a chilled state (as with S3 Glacier Flexible Retrieval)
- 40KB minimum billable size
- 180 day mininum billable duration
- Retrieval takes up to 48 hours

## S3 Intelligent Tiering

Designed for long lived data where the usage pattern of the objects is changing or known.

- Comes with 5 tiers of storage
  - Infrequent Access
  - Archive Instant Access
  - Archive Access
  - Deep Archive
- The tiers are similar to the other equivalent S3 classes, but intelligent tiering will move objects to the different types of storage automatically for you.
  - Monitors usage of objects - if they are not accessed for 30 days, moves the object to lower cost infrequent access tier and eventually to deep archive down the tiers as time goes on.
- 90 day minimum for Archive Instant Access tier (similar to infrequent access class and also has instant retrieval)
  - More than 180 or 730 days without access, the objects are moved down into the archive access and deep archive tiers
    - These lower tiers are OPTIONAL and getting objects back is not immediate
  - If objects in deeper archive tiers become accessed suddenly more frequently, they will be moved back up to higher tiers.

### Billing for Intelligent Tiering

- monitoring and automation cost per 1000 objects.
- The costs for the tiers are the same as their storage class equivalents, but there is the management fee on top of that cost.

# Lifecycle Configuration

A set of rules that consist of **Actions** based on a set of conditional criteria.

- Provide a way to automate changing storage classes or deleting objects over time to optimize for lower costs
- Can apply to a bucket or groups of objects with prefix/tags

### Actions types

- Transition Actions: change the storage class of the object(s) (i.e. from S3 standard to S3 Infrequent Access after 30 days if you know the objects will be accessed less frequently after a month)
  - NOTE: This is not changed automatically based on access - that is Intelligent Tiering and a different service
- Expiration Actions: can delete/expire objects or versions after a time period

### Example use case

- You know objects will be used less over time so you:
  - add transition action to move objects to infrequent access after 30 days
  - add another transition action to move objects to S3 Glacier after 90 days

## Transition action limits

- see [video](https://learn.cantrill.io/courses/1101194/lectures/25997335) at timestamp 4:20
- Waterfalls down the levels (cannot transition UP levels!)
- S3 Glacier - Flexible Retrieval can be transitioned down to Glacier Deep Archive (but not up above to S3 One Zone IA)
- **EXCEPTION**: S3 One Zone IA cannot be transitioned down to S3 Glacier Instant Retrieval, but skip that level down to Glacier Flex and Deep Archive

### Caveats and Warnings

- Be careful about transitioning from S3 Standard down to S3 Standard IA, Intelligent Tiering or S3 One Zone IA with smaller object collections.
  - There is a minimum size cost and this can wind up costing MORE when you're transitioning groups of small objects
- **EXAM** If starting in S3 Standard, you need to have objects there for **30 Days** before you can transition to Infrequent Access or One Zone Infrequent Access.
  - It's fine if you go directly to Infrequent access classes first, but if you start them in S3 standard, you must wait 30 days before transitioning them
- Also another **30 Days** wait if you use a single rule to move from S3 Standard to the IA classes and THEN to Glacier classes. You must wait an additional 30 days before moving from Infrequent Access to the Glacier levels if in a single rule.
  - You can have two rules to make the transition from Standard to IA, then another for IA to Glacier, and then you don't have to wait the extra 30 days between going to IA and Glacier

# S3 Replication

### Two Main types:

- Same Region Replication
- Cross Region Replication

### Configuration

- Replication occurs from a **SOURCE** bucket to a **DESTINATION** bucket
- Configure an IAM role to use for the replication process
  - This role is configured to allow the S3 service to assume it (defined in it's Trust Policy)
  - Permissions given include
    - Read objects from source bucket
    - Replicate those objects to the Destination Bucket
- Replication across buckets is over an encrypted SSL connection

### Same Account vs. Different Account replication

- Both types can replicate buckets in the same AWS account, or in different AWS accounts
- In Same Account Replication, both buckets trust the account that both are in
  - They both trust IAM as a service
  - They both trust the IAM role
    - The IAM role automatically has access to the Source and Desination Buckets as long as the IAM role's permission policy grants that
- In Different Account replication, the Destination bucket, being in another AWS account, does not trust the Source bucket's Account or the IAM Role that's used to replicate the contents
  - **This requires configuring a Bucket Policy on the Destination Bucket (A Resource Policy) that explicitly allows an IAM role in the Source Bucket's Account to replicate objects into it.**

### Replication Options

- Replicate All objects
- a subset of objects
  - Create a rule that adds a filter that filters objects by prefix or tags or both.
- Can select the Storage Class that the Destination Bucket will use (it can be different from the Source Bucket's)
  - **The default is to use the same storage class**
  - Can be useful to make the copy replica cheaper to store if it is secondary data (i.e. could use One Zone Infrequent Access for the replica)
- Can define Ownership of the objects in the Desination Bucket
  - The default is that the objects in the destination bucket retain ownership by the Source Bucket's AWS Account
    - This could cause issues if replicating accross different AWS Accounts (the Destination Bucket Account may not be able to read objects copied to the Destination Bucket because they're owned by the different Account)
    - You can override this so anything created in the Destination Bucket is owned by the Destination Bucket's Account.
- Replication Time Control (RTC): adds a gauranteed 15/50 ? minute SLA on the replication process
  - Adds additional monitoring to see which objects are queued for replication as well
  - This is only used if you have strict requirements from the business for the replication buckets to be in sync within the time window.
  - Exam might ask questions about needing a timed requirement in an application.

### Replication Considerations

- By default replication is not Retro-active
  - Enabling replication on a bucket that already has objects, those existing objects will not be replicated.
  - There is an option to use **Batch Replication** to replicate existing objects, but you need to specifically configure this
- **Versioning must be enabled to allow for replication** on both the Source and Destination Buckets.
- Replication is **ONE WAY ONLY**
  - Adding objects to the Destination Bucket will not be replicated back to the Source Bucket
    - There is a recent optional setting you need to configure if you want bi-directional replication.
- Types of objects you can replicate:
  - Unencrypted objects
  - Encrypted with SSE-S3 objects
  - Encrypted with SSE-KMS objects (requires configuration and extra permissions setup)
  - Encrypted with SSE-C objects (recent addition)
  - Replication only works on objects that are OWNED by the Source Bucket's Account
    - If you have allowed other accounts to insert and own objects in the Source Bucket, those objects cannot be replicated.
  - Cannot replicate System Event objects (only User Event objects in teh Source Bucket can be replicated)
  - Cannot replicate objects in the Glacier or Glacier Deep Archive storage class objects.
  - Delete Marker objects are NOT replicated by default
    - you can optionally add DeleteMarkerReplication setting to enable that

## Why use Replication?

### Same Region Replication:

- Log Aggregation (if you have multiple buckets that store logs for different systems, you can replicate them all to one bucket for central aggregation)
- Sync PROD account data to TEST environments/accounts
- Strict Sovereignty requirements where data cannot leave a region (provides account level isolation by replicating data accross accounts to one replicated Destination bucket - for auditing etc.)

### Cross Region Replication:

- used for Global Resilience to cope with regional failures
- can be used to reduce Latency if you need to data to be closer to customers in different region

### DEMO setting up Replication

- [Video Demo](https://learn.cantrill.io/courses/1101194/lectures/26227536)
- This enables for example, a way to recover from a disaster in one region by restoring data from another region where it was replicated.

  - Create source and destination buckets (make the destination bucket in a different region from source)
  - Enable cross region replication in S3 - see demo at timestamp 7:07

    - Click on `source bucket` > `Management tab` > `Create replication rule`
      - Enter a name, i.e. `staticwebsiteDR` for disaster recovery
      - LIMIT SCOPE or APPLY TO ALL OBJECTS - Note: Rule scope in the options means you can replicate only certain folders/prefixes (instead of the whole bucket if you want that)
    - Destination: For Destination settings to choose the destination bucket, note if you choose from another account you need to make sure the objects in destination have the correct ownership (by default objects are owned by the account in the source bucket)
      - Choose bucket > Browse S3 - Enable versioning if prompted
      - You'll have an option to enable the correct ownership you'll just need to tick if selecting a destination bucket in another account
    - IAM Permissions: Give IAM permissions to the replication rule to read data from the source bucket and write data to the destination bucket (see timestamp 10:15)
      - Create new role
      - Note the new role name will have the name of the source bucket in it if you need to find it
    - Overview of new additional replication options at timestamp 11:51
      - RTC - Replication Time Control - gaurantee replication within a time limit - comes at an additional cost (99.99% of new objects are replicated within 15 minutes)
      - Deleting markers replication: by default if you delete an object in the source bucket, because of versioning, that object is not deleted in the destination bucket.
        - Selecting this option means delete markers are replicated from the source through to the destination.
    - Asked if you want to enable existing objects (select no or yes) - new feature offered in AWS replication.

    Replication is now in place and it could take 5 or 10 minutes from when you add files to the source bucket for them to be replicated in the destination.

# Pre-signed URLS

- Can be used to access private S3 bucket objects using the access rights of the identity which generated the pre-signed URL
  - Time limited
  - Encode all of the authentication information needed in the URL
  - Can be used to upload or download objects from S3
- A way to give another person or application access to an object inside an S3 bucket using your credentials in a secure way.
- An IAM user of the account can specify access to a specific object in a specific bucket that expires at a date and time.
  - While the presigned url is used by another user who has it, they are considered the same IAM user who generated the presigned url.

## Use cases

- Pre-signed URLS are often used when you offload media to S3
- In serverless environments where access to S3 needs to be controlled, but you don't want to run an application layer to broker that access
- Can be useful to remote workers who don't have a AWS account or need access to one specific object.

### Use case scenario (at timestamp 6:00):

- [video](https://learn.cantrill.io/courses/1101194/lectures/26438087)
- A server has information on wildfires in a region and there is an accompanying video for the info hosted on a private S3 bucket
- The server initiates a request to S3 asking it to generate a pre-signed URL for the video object
  - Uses permissions of the IAM user the application making the request is using
- S3 creates a pre-signed URL with authentication information for the app IAM user encoded in it to access the bucket for a short time limited basis (i.e. 2-3 hours)
- S3 returns that pre-signed URL to the server and through to the application user
  - The application on the user's computer then uses this pre-signed URL to securely access the video object

## Oddities/NOTES for Pre-signed URLS

- Can create a pre-signed URL for an object that you have no access to
  - Only requirements for generating a url is you specify a particular object, and a expiry date/time
  - Since the presigned URL was created by you, it will also have no access to that object..
- When using a Presigned URL, you have the permissions that the Identity which generated it has at THAT MOMENT!
  - If you get access denied when using it, that could mean that the identity which generated it currently does not have access to that object, or it never did to begin with
  - Generally this is not a problem because you would create an IAM user for the application which generates the URL and it would have static permissions generally.
- **DO NOT Generate presigned URLs based on an IAM Role**
  - IAM Roles have temporary credentials, while the presigned URLs generated by them can have much longer validity period
    (so you would get access denied because the temporary Role credentials will expire before the URL does)
  - Always use long term entities to generate presigned URLs (usually an IAM user)
