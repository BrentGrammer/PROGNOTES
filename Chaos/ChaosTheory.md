# Chaos and Fractals

- Based on book by David Feldman

## Functions:

A function depends on something for it's output. The input causes the output. Ex, how many hours you spend studying determines the grade on a test.

\*A function must have only one output for every input. There cannot be multiple values for a given input.

Deterministic vs stochastic (stochos, guess). P. 11

Graphical representation of functions, p. 12

\*\*\*\*Iteration: doing the same thing again and again using the previous steps output as the next steps input.
Note: iteration is not repetition. The output is used for the input where in repitition, this does not occur.

Seed: the initial condition starting in an iteration.
Itinerary or orbit: the various outputs in sequence starting from iterating on from a seed.

Dynamical systems, p. 22. Some variable or set of variables that change over time.
Iterated functions are one type of a dynamical system. The system changes over time (outputs)

- iterated functions are one of the simplest examples of dynamical systems.

Types of dynamical systems:
Continuously varying(altitude of airplane)
Discrete (iterated functions that change suddenly at fixed intervals).

### FIXED POINTS:

Fixed point: an input that yields the same output in a function. f(x) = x^2, where x = 1 or 0.

Unstable fixed points and repellers, p. 30. Fixed points where if input is altered even slightly will result in different orbits as iteration continues.
(Orbit being the iteration of the output to the input over and over again)
Ex: 1 as input to the square function above.

Attractors or stable fixed points like 0 as input in the square function. p. 31. Alteration of input does not affect orbit like unstable does. If the input is deviated from zero, the orbit will tend towards zero.

This is more common and what we expect to see in simulations and real systems.
Example of attracting fixed points of 1 in p. 33-34 for square root function.

Neutral fixed points, p. 31. P. 48 is an example. Oscillating around fixed point but neither attracted or repelled.

4 TYPES, see p. 47
Attracting
Repelling
Attracting with oscillation (it goes above and below the fixed point back and forth until getting closer a the oscillation narrows.)
Repelling with oscillation
Cycle with a period (it goes above and below a fixed point in intervals, see ex. On p. 79, fig 9.8)

Time series plot is a good way to see behavior of orbits for given inputs, i.e. whether they attract or repel from fixed points.

Interesting way to quickly determine behavior of an itinerary to a fixed point on a graph, p. 39.

Graphical representation of an attracting and repelling fixed points 0 and 1 on p. 41. Fig 5.9

Graphical slope indications of fixed point quality, p. 48-49

Exponential function example of rabbit population at a time of iteration. P. 55

Book to check out, Modeling Nature by Kingsland. (focused on history of population modeling and bioligists)

\*With modeling techniques such as the logistical equation for iterating over different population growth rates, the goal is not to produce realistic results but to get an understanding akin to a sketch or caricature. P. 64.

Science, origin and aspiration:
Laplace's demon, p. 70. If Newton is right and the universe is deterministic, then an intelligent that knows all objects in the universe and all forces at a certain moment could predict the future actually. Question of free will etc.

3 main parts to making a good prediction of the future in science:
Measurement
Knowledge of the rules laws of nature
Computation
Aspiration is that if we just measured more carefully, knew a little more, and made more powerful computers we could solve mysteries of universe. The goal is to be Laplace's demon

Final state diagram, p. 78. A line of the range of possible results and the final fate of an orbit.

Different periodic behavior examples, p. 81-82. Sometimes it takes a while and a number of iterations before a period is established. Also periods can be long (I e. 8).
Note a period of 1 = fixed point

Aperiodic behavior, p. 84. There is no repeating pattern or period (fixed point either).
Way of checking patterns on a computer, p. 84. Record value at 1 million iterations. Check each next iteration for the value to detect if a pattern has finally been established.

Logistic equation: f(x) = rx(1 - x)
Predicts population growth at rate r where x is the maximum percentage(decimal) threshold of sustainable population.
The output is maximum of 1, representing decimal value of percentage of the max threshold population the current population is.

## CHAOS, p. 85

Ex. Is r=4 in the logistic equation. This r value causes chaotic behavior.

Non repeating and apparently random phenomenon do not need complicated equations or external randomness to be produced. They can be generated by simple, deterministic rules.

#### Chaotic systems have 4 properties:

1. Dynamical rule (the function) is deterministic

2. Orbits are aperiodic (never repeats)

3. Orbits are bounded (limited to a range, not infinity, i.e. the logistical equation results have lower/upper limits of 0 and 1.

4. System has sensitive dependence on initial conditions. The butterfly effect, very small chance in initial condition leads to very large change in the orbit in a short period of time.

Chaotic systems are deterministic sources of randomness.

The aperiodic behavior itself is not interesting if the function leads to infinity, i.e. f(x) = 2x. This is not chaotic and the orbit goes to infinity and it's no surprise it doesn't repeat. In chaotic systems the aperiodic behavior is relevant within bounds.

Note, you can use a computer program online to make orbit plots and iterate at http://chaos.coa.edu
P. 87.

SDIC (Sensitive Dependence on Initial Conditions), p. 93

- a lesson is that systems that have SDIC are impossible to predict for anything other than a short term. See example on p. 92 of a minor difference in initial conditions resulting in wildly different orbits after a number of iterations.

For a system to have SDIC, virtually all the initial conditions must have the property that a small change leads to large changes long term. Very rare conditions can exist that have unstable periodic cycles. P. 93

Formal definition of SDIC is on p. 94. Basically two orbits are a certain amount apart after n iterations.
You should be able to find another starting condition that is not farther away than y amount from an original initial condition whose orbit eventually ends up more than x amount away from the original condition's orbit.

For a system to be chaotic, it has to scramble orbits so much that nearby any initial condition there are other initial conditions that get far away from it.
A chaotic system pulls nearby orbits apart.

Example of opposite: logistical equation with r of 3.2. the orbits of multiple starting conditions eventually all pull together in a periodic cycles of 2 by the fortieth iterate. This does not have SDIC. P. 95

In chaotic systems multiple orbits with nearby initial starting positions pull apart. In systems without SDIC they pull together.

Lyapunov exponents can express the behavior or speed in which orbits are pushed apart. The largerit is, the faster the orbits are pushed apart and the more unpredictable a system is. P. 97
\*\*\*Beware systems that have close starting conditions whose orbits push apart quickly! They are more unpredictable.

#### Stretching and folding, p. 97

Chaotic systems orbits must be bounded and stay within a range. This means that the distance between initial conditions orbits stretches and folds to remain within the bounds (it cannot stretch indefinitely).

Example of non bounded system that is not chaotic (needs bounds) but has SDIC is f(x) = 2x. The bounds is infinite, but small difference in initial conditions result in large difference in orbits

The logistical equation can be thought of as a general equation for chaos. Almost all chaotic systems show similar behavior to that produced by this equation.

Limitations of using computers to calculate orbits of very fine decimals (1/11). The computer can't store the entire decimal and calculates an orbit for a rounded version. Eventually the orbit down is completely off (SDIC) if using that in the logistical equation with r=4.
When dealing with computing orbits for small decimals, you cannot rely long term on accuracy.

Shadowing lemma, p. 100. The computed orbit that strays from acurrately mapping that of the initial condition due to round off errors actually closely approximates the true orbit of some other starting condition well. So it can still offer some insight into the behavior of a dynamical system.

#### Bifurcation

- Bifurcation diagram, fig. 11.2, p. 106. Plot of r values and final states (where values end up in orbits). Provides a way of summarizing in picture all the possible stable, long term behaviors of a system. It summarizes the behavior of various orbits (are they periodic, aperiodic, how many period cycles? Etc)

- Bifurcation: a sudden, qualitative change in behavior of orbits as a system parameter is varied continuously. P. 106
  Example, orbits that are periodic for a range of parameter r, but suddenly change to aperiodic orbits at a certain r parameter value.

Periodic windows, p. 107. Regions of behavior where periodic orbits occur in ranges between regions of aperiodic behavior.

Transitions from periodic behavior to chaotic behavior examples, p. 108-110. Periodic windows occur where as r increases , the period cycle doubles and doubles until chaotic aperiodic behavior returns.

There can be patterns in very small ranges of r, that are similar to periodic windows and periodic doubling that occur in other larger ranges of r. P. 110. Self similar characteristics.

There is at least one periodic window in any interval of r values that contains a chaotic region on a bifurcation diagram.
Chaotic regions are punctuated with periodic windows (brief moments of order)

Rhyme or reason to the order in which periodic windows occur in chaotic systems such as logistic equation: sharkovsky ordering, p. 111

Interesting example of this ordering in the logistical equation on page 111. As you encounter periodic windows in the bifurcation diagram, some of the periodic windows will repeat and recur. But as soon as you get to a window that is new, it follows the parttern of reverse sharkovsky ordering.
So there is a overarching pattern or sequence, but in between each member of the sequence there are reflections or recurrences of the periodic windows seen before!

Summary of the pattern seen with the logistical equation as shown by it's bifurcation diagram, p. 111.
"Periods double successively and explode into chaos. Within chaos, periodic windows suddenly emerge. These periods then double successively and again burst into chaos."

You can experiment with bifurcation diagrams with software programs at http://chaos.coa.edu
P. 112

### Chaos Defined

- aperiodic, bounded orbits that have sensitive dependence on initial conditions and are generated by a deterministic system.

- \*Bifurcation diagrams are a good way to analyze the chaotic behavior for a given function.
  - Bifurcation diagrams plot the parameter value of a function on the x axis and the final states of the function when iterated on the y axis.
  - A bifurcation is when a period changes on the plot.

### Universality: properties of a transition from order to chaos that are the same across large classes of mathematical (and physical) systems. P. 115.

\*\*The ratio of successive periodic region widths winds up at 4.669 on almost all bifurcation diagrams that exhibit a period doubling route to Chaos. P. 128

\*The period-doubling route to Chaos: common pattern of behavior where as a parameter gets larger on a bifurcation diagram, the period of orbits doubles leading to a chaotic region.

#### Feigenbaum's constant

- p. 120. If you measure the width of a periodic region in a bifurcation diagram and compare it to the width of the next doubled periodic region to get a ratio, as doubling of the periodic regions increases, that ratio approaches a fixed number: 4.66920160...
  Or 4.669

This ratio constant appears in the bifurcation diagrams of the logistic, sine, and cubic equations (and almost all others)
\*Note Feigenbaum's constant holds true when the number of periodic doubling gets large (later in the bifurcation diagram), but earlier periodic doublings have a ratio relationship that is close to the number generally.

This is a quantitative similarity between many functions that have period doubling periods on the route to Chaos. As one approaches the transition to chaos, the only possible ratio for the lengths of successive periodic regimes is Feigenbaum's ratio constant.

This behavior across functions in physics is known as "universal".

This ratio is observable in physical phenomenon.
Example of a dripping faucet, p. 121
As the flow of water increases drops will fall and splash in periods that double.
If calculation the width of these periods and comparing them successively, we find Feigenbaum's number.
Other examples include convection rolls in fluids and electric circuits where this is observed as a parameter is altered.

All parabola like one dimensional functions that undergo a period doubling route to Chaos exhibit the same quantitative behavior showing Feigenbaum's constant. (They must have a second order maximum bound like the logistic equation that shows as an upside down parabola on a graph, p. 120).

Renormalization, p. 125. Normalization refers to the process of setting a length scale, coding b units of measurements and scale on a graph. Renormalizing is when you zoom in and change the length of the scale.
All possible magnifications are known as the renormalization group.
Ex, any curve will look like a straight line under successive renormalization.

As you renormalize and zoom in on a bifurcation diagram near the transition to chaos, you see the same image, pitchforks with pitchforks (similarly if you zoom in on a straight line, you always see the same image, a straight line.)

A super attractive fixed point, p. 125. r=2.0 in the logistic equation. Nearby orbits are pulled toward a fixed point of period 1 at the fastest rate compared to other r values that also have the same period behavior (period-1 in this case).
You can have super attractive fixed points of differing periods, i.e. period-1, period-2 etc.

A renormalization operation could include inputting a function to return a function that produces a shape similar to another functions output shape, p. 126-127.

You can iterate this rescaling and these function transformations and find that initial conditions will be pulled toward a fixed function, (similar to a fixed point as with iterating a function normally considering all initial conditions). Almost any initial condition will end up at the same function. p. 127

\*Any equation that is parabola like will get pulled toward a universal function upon rescaling. It explains why many different functions have the same value for the ratio of successive periodic cycle widths (Feigenbaum's constant).
Similar to how if you continuously zoom in on a curve, you eventually see a straight line, even if dealing with many different curves. The line is an attractor.

A function similarly has a universal attracting function that you wind up with as you renormalize, provided that the initial function has a single maximum and that this maximum is parabola like.

### Statistical Stability of Chaos

A histogram plot can show the relative frequency of different x values along an itinerary (y axis) that fall within ranges (x axis)
Ex. P 132.

\*A histogram is a final state diagram with an added dimension showing what values occur but also their relative frequency.

Histograms are a good tool for analyzing behavior of chaotic orbits.

See histograms for the logistic equation with r=4.0 with two separate initial conditions. They have the same shape even though both orbits are chaotic. P. 133-34

Ergodicity, p. 135.
An orbit is ergodic if it gets arbitrarily close to any point on the interval in which it roams (the interval is 0-1 in the case of the logistic equation).
If you choose any point in the interval then the orbit will come very close to it at some point. Note it is impossible to actually hit every point directly since there are too many

A key characteristic of ergodic orbits or ergodicity on general is that there is a path outcome that is guaranteed (all points hit regions at some point in some statistical predictable way for ex.)

"Almost" all initial conditions of the logistic equation are aperiodic. This means technically that there is practically zero probability of orbits that are periodic (though technically possible).

Ergodicity illustrates how chaotic an orbit is since it shows how it wanders close to anywhere on the interval. On the other hand, the fact that it explores every tiny region gives it a certain predictability, and histograms are the same for almost all of v these orbits. The trajectory of the orbits will be different, but each will spend the same fraction of time in each region of the interval. P. 136

\*Thus histograms can provide a predictable and regular feature of a chaotic system, ex. The logistic equation.

You can also consider iterating an equation a few times from a million different initial conditions and plot a histogram of the results and points hit from all of them. It will look similar to the chaotic hospital of a single orbit.
Invariant distribution,p. 137. ???
All initial histograms are pulled towards it.

The histogram is a statistical structure, it captures the average properties of the orbit.
\*The phenomenon of chaos is statistically stable provided there is a ergodic orbit.

For many chaotic dynamical systems there can be only one attracting fixed histogram or invariant distribution.

\*\*\*The main point of all this is that even though it is hard to make long term predictions of an orbit of a chaotic system, it is possible to predict the time an orbit spends between specific intervals, for example, in the logistic equation, since the orbit for r=4.0 is ergodic, the time percent an orbit hits between 0.1 and 0.2 is 9.03%, regardless of the initial condition p. 138
The butterfly effect does not affect this prediction since it is statistical.

Example in weather is that predicting weather more than a few days out is hard, but average rainfalls per month and statistics over a long time is easy.

### Chapter 14

Symbolic dynamics, p. 141. Another tool to use for studying dynamical systems.
The point is to encode numbers in an orbit to something simple (since there can be an infinite number of possible points on a chaotic orbit).
Ex. For the orbit of logistical equation where r =4.0, you could take all points below 0.5 and assign the symbol L and all points greater than 0.5 and assign the symbol R.

Ex:. RRLRRLRR... for orbits with initial condition of 0.613.

Thought: you could sign ranges of the contpur of rock formations to symbols (notes) and retain the "information" as opposed to plotting each point individually.

Analyzing chaotic systems this way is still useful even though a lot of information is lost using the symbols.
The reason is the two representations are topologically conjugate.
Topological conjugacy means that there is a similarity between the two systems, they both reflect periodic cycles and that the system is chaotic. P. 142.

Additionally you can determine the range of the possible initial condition from a sequence of symbols alone. The more symbols you have further down the orbit, the smaller the possible range is. P. 142-3

The sequence of Ls and R symbols create a partition which is range of possible initial conditions that would produce that sequence.
A generating partition is arrived at with longer and c longer sequences of the symbol to narrow down exactly what initial condition produced them.
Just as the initial condition contains all the information of an orbit (since it is a deterministic system), a dynamic symbol sequence contains all of the as well (the periodic cycles, etc), p. 143.

Randomness vs. Stochastic, defined, to p. 144.
The possible sequence of all L-R patterns of the logistical equation dynamic symbols when the orbit is chaotic all have an equal frequency of occurrence making the orbit as "random" as a coin toss.

A Stochastic system is nondeterministic and does not produce the same output given the same input as a deterministic function does. There is something in the function that causes different results given the same input.

\*This is different from the random patterns produced by a deterministic system. In this case random means a paternless outcome of the outputs.

Random sequences are "incompressible". You cannot generate the sequence with a short algorithm, i.e. "alternate L and R forever..."
Compressing it means finding a representation of it that is shorter than the original.
P. 145.

To technically define what makes something random, we need to know what machine will produce the algorithm(since different machines could produce shorter or longer ones to compare with the sequence to determine it's compressibility). We should use a universal touring machine, p. 146.

Random results from a deterministic non-stochastic process are sometimes called, pseudo-random since they are not derived from stochastic processes.

Explanation of how using the logistic equation with an initial condition specified is not actually a shorter algorithm making the chaotic orbit compressible (and therefore not random) on p. 146-7.

\*\*\*\*The key insight of chaos theory is that a series of apparently random observations does not necessarily arise by chance, via stochastic processes, but can also arise from deterministic systems!

Thought: is coin toss truly random since the laws of physics are deterministic? P. 147 offers an explanation that quantum mechanics allows for true randomness since randomness at the quantum level can be amplifief by SDIC leading to everyday phenomenon that are truly random.

\*There is a alternative view to quantum mechanics called Bohmian mechanics, which is deterministic as opposed to nondeterministic quantum theory, but it proposes signals can travel faster than the speed of light which violates einsteins theory of special relativity, so most scientists prefer standard quantum theory.

Linear,nonlinear and reductionism, p. 148

Reductionism: the belief that the way to understand a complex object or phenomenon is by understanding the properties of it's parts.

A flaw in reductionism is denying the importance of interactions between an objects constituent parts and failing to recognize that an objects properties may depend on it's context.

Despite this flaw of using reductive thinking, it seems unavoidable to gain knowledge in that we humans need to cost since portion of the world to focus on that is smaller than the world itself.

We have to study parts of the whole by necessity, but it is dangerous to imply that the part is the whole.
There can be an overglorification of studying the micro sciences (genes/particle physics) vs. the macro sciences(ecology, weather).

Linear functions: solutions can be composed to produce other solutions, exof ice cream points in cart with consistent price per pint, p. 150-1.
Linear systems are decomposable, the parts do not interact and can be combined.
Linear systems have solutions made up of building blocks that don't interact.
\*The modularity of the system limits the complexity of solutions such that it is impossible to have orbits that are aperiodic and have SDIC.

Nonlinear systems: parts do interact and you cannot decompose parts of the system. Example of ice cream pints that can change price based on volume purchased. The presence of other pints changes the price per pint.

In order for a system to be chaotic it must be nonlinear.

\*\*\*Good summary of chaos theory on p. 152-153.
Good reading recommendations including chapters 2,3,14 of flakes computational beauty of nature book which you have.

## Fractals

- self similar geometric objects.
- Fractals are self similar, small parts of a fractal look like larger parts. Ex, a tree.
- Fractals dimensions: degree of branching and what extent to which the features at succession scales are similar.

P. 157, we could describe a winding river or branching trees as a collection of line segments arranged a particular way, but this would be missing the essence of the shape we are trying to describe.
(Thought: in music, literally describing the shape or points or contours exactly might be missing the essence as well).

Fractals structures that are self similar in a non trivial way (a straight line is not a fractal).
The parts don't have an average size (since the parts can be all kinds of sizes).

In real fractals in nature you cannot zoom in infinitely or out infinitely and still see a self similar structure. This is only true for fractals which are abstractions or idealizations we can use which are useful.

The self similarity dimension, p. 164. How many self similar pieces of an object fit inside a large piece?
Num small copies = (magnification factor)^D where D is dimension.
Ex, square is 9 = 3^2

Can solve this equation using logarithms, see appendix A.4

Note that dimension can be a decimal and not always an integer in fractals. P. 166
This means a fractal can have aspects of more than one dimensional profile, i.e. the cantor set has a dimension in between 0 and 1, so it has aspects of both a one dimensional line and a zero dimensional point.

Magnification factor = how big is each small copy of an object compared to the larger one?

\*The point of using dimension as a measurement is that it relates the number of small copies of an object to the change in scale, or magnification factor. This relationship is the same for any scale change we consider. It is in this sense that D (dimension) captures what stays the same across different scales.
I.e. as you increase or decrease magnification to yield different numbers of the small self similar objects included in the adjusted scale, dimension remains constant.

The non-integer dimension in fractals describes the scaling behavior of an object or pattern. Specifically, it describes how the object changes as its size is increased or decreased. For example, if we have a line segment of length 1, and we scale it by a factor of 2, we end up with a line segment of length 2. The scaling factor is 2, and the dimension is 1, because the length of the line segment is proportional to the scaling factor (i.e., it doubles in length when scaled by a factor of 2).

Topological dimension,p. 169: represents or intuitive dimension of an object, i.e. a square is 2 dimensional, a cube is 3 dimensional, etc.
The cantor set's topological dimension is 0 since if we remove infinite number of line segments, we wind up with points. In the sierpinski triangle, if we remove infinite triangles we wind up with a dimension of 1 since what remains is tiny line segments.

One more specific definition of a fractal is a geometrical object whose self similarity dimension is greater than is topological dimension. I.e. the cantor set has a self similarity dimension of 1.585 and a typical dimension of 0.
P. 169

#### A broader definition of a fractal:

1. exhibits self similarity across a range of scales. This self similarity can be exact or approximate.
2. not well described by usual geometric forms like circles, cones and lines
3. the self similarity or some similar dimension is larger than the topological dimension.

Note: there are variants on the self similarity dimension including capacity dimension, box counting dimension and the Hausdorff dimension.

\*The basic idea is that dimension measures how the bulk or volume of the shape scales.

Famous fractals: menger sponge, sierpinski triangle, Koch curve, cantor set

### Random Fractals

P. 173

Fractals can be generated by a deterministic procedure: an exact rule is followed at each step of an iterative process.

Fractals can also be generated by mechanics with some randomness or irregularity by adding noise to the process.

P. 174, the Koch curve can be thought of as similar to a coast line: it has inlets, which have inlets on them which have inlets on them etc. It is an exactly self similar fractal though.

It is too symmetric to represent a realistic coast line, though. If randomness is introduced into the generative process (of bending a seed line segment in the middle iteratively) so that each time a bent segment replaces a line segment, 50% of the time it bend upwards, and the other half of the time it is bent downwards, this produces a random Koch curve.

The random Koch curve is not unique and the generation rule is not deterministic, since we don't get the same outcome everytime.

\*Statistical self similarity: the property of inexact self similarity in fractals, p. 175.
Random fractals are not made of exact copies of itself, but of smaller parts that have the same statistical properties of the whole.
The random Koch curve is "statistically self similar".
Nb: it is not usually the case that a random fractal has the same dimension as it's deterministic counterpart.

The random Koch curve approximates more closely a real coast line and is similar to a crack in the pavement, a rock outcropping, or a turn piece of paper.

\*\*\*Adding a little bit of randomness to a regular fractal produces shapes that are strongly reminiscent of other physical and natural objects.

Complicated shapes can be generated by a simple rule, the rule can involve randomness and not be deterministic, but it is simple nonetheless.

- irregular fractals, p. 176. A variation on regular fractals that are not random, but generated by a deterministic asymmetric or irregular rule.
  Ex, a sierpinski triangle that at each step has a slightly tilted triangle removed, p. 177.

\*\*\*Fractal forgeries, p. 178. Random and irregular fractals are extended and refined to produce images resembling natural landscapes.

Software: Fracplanet by Tim Day. Http://www.bottlenose.demon.co.uk

Ex. Image on p. 178 was generated by a procedure similar to the random Koch curve, but in two dimensions.

Software: you can make irregular fractals with the fractal curve demo program by Christian desrosiers at http://profs.etsmtl.ca/cdesrosiers/software.html

The chaos game, p. 179-180. Coding a point anywhere in a triangle, then rolling a dice to determine which corner a,b,c to move to but only half way.
Counterintuitively, This stochastic (since an element of chance is introduced to the rule) rule results when iterated in an orbit that produces the sierpinski triangle.
This shows that a random dynamical system can give a deterministic result (chaos theory, but in this example, a nondeterministic function iterated produces a deterministic result!).

Interestingly, if sequence is introduced to the chaos game, removing chance, then the result is a three period cycle of the same three points in the long term orbit.  
\*\*Randomness is required to produce the deterministic result of the sierpinski triangle long term.
(The order in which the orbit visits the sierpinski triangle is random, but the fact that it eventually produces it is not!) P. 181.

The Collage Theorem by Michael Barnsley, p. 182.
Given any shape, one can make a chaos game that generates it.
Note that almost any shape fractal or non fractal can be generated by the chaos game.

A generalized chaos game can use more complex rules with affine transformations of a given shape. A affine transformation is any geometric transformation that keeps parallel lines parallel. Using these transformations (for example, scaling, rotation, reflection) and moving the shape would be the process.
Each move in the generalized chaos game consists of a random or stochastically generated affine transformation.

P. 183-4 has reading recommendations.

### Fractal Dimension

Box covering dimension, p. 188, can be used to measure dimension of fractals that are not exactly self similar or more generally measure dimension.

- As the number of squares (N(s)) to cover a surface increases, the size (s) of the length of the side of these squares decreases.

Formula: N(s) = k(s)^D

- k is some constant and D is dimension.

This measurement is more accurate as the box size gets smaller and smaller.

The box covering b dimension is only an estimate and difficult to get accurate results. The box size needs to be as small as possible for more accuracy.
It is a flexible way to estimate the dimension of irregular fractals.

Running average: p. 197, calculating the average on each iteration so far.
\*The running average fluctuates (going up or down from a point) less and less as more data is collected.

St Petersburg game, p. 200. More iterations do not settle toward a running average point, but are fractal like with periods of selling and larger jumps up.
St. Petersburg paradox, the average winnings of the st Petersburg game are infinite. P. 203

Calculating probability of successive coin toss patterns, p. 202. Probability of one toss x probability of another toss, ex. 1/2 x 1/2 = 1/4 for TH outcome.

\*For some phenomena the average is meaningless. You can take the average of the first 100 iterations of the st Petersburg game and get a definite measure, but it is only meaningful for those 100 iterations, not the entire game if more data is gathered. P. 204

Similarly fractals don't have a meaningful average of considering the average size of it's components, i.e. a triangle in a serpienski triangle.

-- chapter 20 power laws p. 208.

In fractals its more useful to look at the distribution of sizes.

Note, normalizing histograms and probability density graph also known as a distribution function, explained in appendix B.

The Gaussian distribution is the same as a normal distribution, also a "bell curve". Formula for calculating from a histogram is on p. 207 and explained in appendix B.

Variance and standard deviation determine how far apart values on the x axis of a distribution graph are.

\*In a normal distribution about 68% of the data is expected to be within one standard deviation of the mean (center of the bell curve).

Central limit theorem, p. 210. A distribution of the sum of random variables approaches a normal distribution as the number of variables in the sum increases.
This holds for a large number of any random variables!
Note, this theorem only applies to a sum of random variables, and not for ex. A product of random variables.

Explaination of additive factors/variables on p. 211. The height of a person is determined by multiple factors, genes, nutrition etc, each contributes a bit of height additively. The sum of these variables lend to the central limit theorem since they are additive.

If we have a quantity influenced by different factors, generally they are additive so Gaussian distributions are ubiquitous.
In general for a lot of datasets there is an average and a relatively small range of variance from that average.

Poincaré quote, p. 212:
_"The scientist does not study nature because it is useful; he studies it because he delights in it, and he delights in it because it is beautiful. If nature were not beautiful, it would not be worth knowing, and if nature were not worth knowing, life would not be worth living."_

### Other types of distributions, p. 213-14:

- power law distribution - decays slowly (a high value curving towards a low value)
- exponential/geometric distributions: decay very rapidly, used to describe wait time between random events for ex. These arise when there is a fixed probability at every time interval that an event occurs (longer wait times are increasingly uncommon(?))

Power law distributions decay much more slowly than exponential distributions.  
Long tailed (fat tailed) distributions are the result of power laws since they plot on a graph far to the right.
Long tail distributions are those that decay more slowly than exponential distributions. P. 215

\*\*In long tail distributions (from power laws) there is a relatively large probability of an extreme event. In short tail (exponential and Gaussian) distributions, there are vanishingly small probabilities for extreme events.

Power law distributions are similar to fractals in that they are self similar or scale-free (you can zoom in out and it looks the same)
Other distributions are not scale-invariant.  
Power law distributions are exactly those that are scale-free. They are the signature of fractals and vice versa.
These are also called scaling distributions.

Power laws distribution or something like it can account for many phenomena,p. 218.
Fractals are easily generated by deterministic and stochastic processes and so are abundant in the world. The same is true for power laws since they are fractal like (the production of self similar structures is apparently ubiquitous due to ease of generation).

There are many ways to generate power laws as there are with fractals. They are all around us.

You should be skeptical about datasets that appear to be a power law distribution, they are tricky to identify. The main feature of interest is the long tail that changes how probable extreme events are compared to Gaussian and exponential distributions. P. 219.
See further reading section for suggestions.

### Infinities

P. 229, infinite numbers between 0 and 1 are uncountable due to the diagonalization argument. It's possible to generate a number that is not in an infinite set of numbers.
Uncountable infinities.

## Julia Sets

Notation of ranges:

- Internal notation, [a,b] - inclusive range, a and b are part of the range
- (a,b) exclusive range - a and b are not part of the set in the range

### Filled Julia set:

- the set of initial conditions for function f that does not tend to infinity when iterated. Ex, [-1,1] for f(x) = x^2 (the squaring function). P. 238
- The initial conditions in the set could lead to a periodic or chaotic orbit, it doesn't matter, it just matters that they do not go to infinity and are bounded.
- A Julia set is the collection of points for a function that remain bounded when iterated by that function. P. 239
- To test if a point is in a Julia set for a function, start iterating from it and see if the orbit goes to infinity. If it does, then it is not in the Julia set, otherwise it is.

_NB: throughout the chapter filled Julia Sets are referred to as Julia Sets. The interest is the in initial conditions that do not tend towards infinity._

### A Julia Set (i.e. not a Filled Julia Set)

- The Julia set is the boundary of the filled Julia set, ex. -1 and +1 from the above square function example. It is the boundary between two regions: initial conditions that fly off to infinity and those that do not.

### Complex Numbers

P. 241
The square root of -1 is an imaginary number.
Since there is no real number that can be the square root of -1, we make a new number up to represent it, for ex. i
i^2=-1

These are called imaginary numbers. Real numbers are those between negative and positive infinity.

While you can't find an imaginary number of physical objects, there are physical phenomena that are described by complex numbers. Ex is AC circuits.

z is commonly used to represent a complex number. A complex number has an imaginary and real part.

A generic complex number consists of a combination of real and imaginary numbers.
Ex: z = a + bi, or something like z = 3 + 4i

There is a real part and an imaginary part to complex numbers.
The imaginary part in the above equation is 4, not 4i.

Arithmetic and algebra of complex numbers on pg. 242-243

Real numbers can be represented in the real number line, but complex numbers can be vizualised on the complex plane.  
We plot the real part of a complex number on the horizontal axis and the imaginary part on the vertical axis. P. 243.
These are Cartesian coordinates.

Another way of specifying the location of a complex number on a plane is by using it's polar coordinates.
The distance of the point from the origin usually represented as "r"), i.e. zero, and the angle of the distance line from the origin(represented by theta)
P. 244

Multiplication via polar coordinates, p. 245.

### The Julia Set

When iterating complex numbers they can tend towards infinity in multiple directions (i.e. not just positive and negative as with real numbers).
A complex number can be said to tend towards infinity if it's r value (distance from origin in polar coordinates) gets larger and larger. P. 250.
(The orbit gets farther from the origin)

NB: the origin is the point on the complex plane where the real and imaginary part of the complex number is 0 and the x and y axis intersect (0,0)

Can explore Julia Sets for f(z) = z^2 + c
I e. C=0 (makes circle), c=-1 (p. 252)

Some Julia Sets can be disconnected and have spaces in between the points, p. 253 for f(z) = z^2 + 0.84i
I.e. \*\*\*you can have chopped up shapes that in aggregate are self similar."fractal dusts"

Coloring Julia Sets, p. 253. Julia set is usually one color, black. Other points outside of Julia set tend towards infinity at different rates which determines the color.

Software for colored Julia set, p. 254. http://aleph0.clarku.edu/~djoyce/julia/juliagen.html

According to the book on p. 250, there was no quick way to determine points in the Julia set other than trying out many initial conditions and seeing which ones tend to infinity and which ones don't.

Plot with python: blbadger.github.io/julia-sets.html

## The Mandelbrot Set

for Julia Set of f(z) = z^2 is a circle.
Using that as a base we add or subtract "c" to get other sets and variations.
f(z) = z^2 + c

How to categorize or classify Julia Sets?
One dichotomy between different Julia Sets is that some are connected forming one contiguous shape and others are disconnected (sometimes called dusts).

Note on dusts: if you zoom in on them you find that the set has a structure like the cantor set, infinite but disconnected points. What appears connected is actually disconnected when resolution is increased.

The Mandelbrot Set: set of all parameter values for "c" for which the Julia set of f(z) = z^2 + c is connected (a contiguous shape and not a dust).
P. 259
Simply put, it is a listing of all c values that have a connected Julia set (i.e. when plugged into the formula, all initial conditions not tending to infinity make up a connected Julia set, not a dust, and they have a bounded critical orbit)

Critical orbit, p. 259: the orbit of the initial condition z=0.
(I.e. in the eq. f(z)=z^2 + c)

\*\*\*Quicker method of determining whether Julia set is connected(i.e. in the Mandelbrot Set) or disconnected: if the critical orbit does not tend toward infinity, then the Julia set is connected. If it tends toward infinity then it is disconnected.
P. 259

\*Julia Sets that are connected always contain the origin (the point z = 0 + 0i) so the orbit of the origin must be bounded (not tend to infinity to be in the set).
Julia Sets that are disconnected never have the origin point included (orbit of 0 tends to infinity)

Therefore, if the critical orbit is bounded for a particular value "c", then that "c" value is in the Mandelbrot Set, otherwise it is not
P. 260

As you zoom in on the Mandelbrot Set you eventually see baby Mandelbrot sets indicating it is a fractal. Unlike Julia Sets or other fractals, the baby Mandelbrot sets are not exact copies of the original but slightly different.
There are slightly new shapes and forms and variations on there structures as you zoom in.

Colorizing the Mandelbrot Set, p. 261-2.

P. 264-267, for every point on the Mandelbrot Set there is a corresponding Julia set. The Julia set gotten from taken the "c" value from a point will resemble nearby structures in that part of the Mandelbrot Set(i.e. the antennae or number of spokes corresponding to arms on the Julia set).
Also, the critical orbit of the Julia set will approach a period of the same number of branches/arms of the structure.
3 arms -> z^2 + c where initial condition is 0 will approach a periodic cycle of 3.

Every bulb on the Mandelbrot Set has a period associated with it. Zoom in and look at the branches or spokes coming off the bulb and the period will equal the number of branches. P. 267

Further reading, p. 268
Website to explore Mandelbrot Set, p. 268
http://homepages.inf.ed.ac.uk/wadler/mandelbrot-maps/mmaps.html

## Two Dimensional Systems

P. 274
One dimensional example is the logistical equation f(x) = rx(1 - x)
(R is a parameter, x is the input and output, 1 variable.

Two dimensional systems have two inputs and two outputs, (i.e. two variables, x and y
It takes two numbers and returns two numbers.

The output for y depends not just the input of y but also the input of x.
\*Not the same as two separate functions, process should be seen as one multi input and multi output function.

You have two initial conditions.

Functions/equations can be referred to as maps.
I.e. the "logistic map" instead of the logistic equation.

Example of a two dimensional map: the Hénon equation,p. 275.
x = y + 1 - ax^2
Y = bx
(a and b are the parameters, like r is the parameter in the logistic equation).

Hénon map - created to simplify realistic models of the physical systems being studied which were unwieldy.

Final states and itineraries for 2 dimensional functions are plotted on x and y plane, p. 277, fig. 26.5

### CHAOTIC ATTRACTOR (P. 279)

Using two chaotic orbits in the Hénon equation when a=1.4 and b=0.3, with initial conditions of 0.6
Plotting the x and y itineraries of the chaotic orbits on a graph produce an orderly image, boomerang like shape, shown on p. 279.

This is a result of plotting simultaneously the x and y time series for a chaotic dynamical system.
Even though the orbits are chaotic the resulting plot appears as an orderly shape.

There is a relationship that becomes apparent when the x and y orbits are plotted together even though individually they are chaotic aperiodic orbits. (On the graph, each iteration is plotted with a point for it's x and y value)

\*If you zoom in on the plot graph you see it is fractal considering of some double line patterns that are similar as you zoom in. P. 280

The boomerang shape is called the Hénon ATTRACTOR. It is fractal and consists of a self similar structure of folds.  
If you plot orbits of x and y using the patterns a=1.4 and b=0.3, the points will land somewhere on the ATTRACTOR (boomerang shape). P. 283, fig 26.18
Almost all initial conditions orbits are pulled too this shape using those parameters.

If initial conditions start away from the ATTRACTOR, they will get pulled toward it.

The Hénon attractor is a strange attractor: generally these are attractors that are fractal in structure and on which the dynamics are chaotic.

P. 283
The dynamics of a system with a strange attractor are fully chaotic, unpredictable, but they're is an order to them.
The system is constrained despite being chaotic. In the long term the outfit never repeats but it also never strays from the attractor.

The reason they might be called strange is that even though the system is chaotic the attractor shape is robust and stable as almost all orbits are pulled too it regardless of where they start.

Strange attractors occur commonly in two dimensional discrete systems, ex 2 dimensional iterated quadratic functions.

## Cellular Automata

- ch. 27, p. 287

### Cellular Automaton

A dynamical system that has a large number of discrete variables arranged in an array or grid. These variables are then updated at discrete time steps via a local, deterministic rule.

Binary Cellular Automaton: where the variables in the system can have only 2 possible values (on/off, black/white, etc).

State: the state of a CA system is given by discussing all the values of all the variables.

The variables in the system are called cells.

At each iteration the state is filled in based on the rule which uses the previous state to determine the next
A rule could be that based on the value of the current cell and the one to the left and right of it, that neighborhood(group of 3 cells) determines the value, see ex on p. 288

Periodic boundary conditions: to determine the neighborhoods for cells at the edges, we imagine the state wraps around itself. P. 288

Starting what a center automation is means to state what the rule is.

Cellular automata defined: a type of dynamical system in which a grid of discrete variables are updated at discrete time intervals according to a deterministic rule.
It has a initial condition (the first row of cells) and a function (the rules to produce the next row)

Space-time diagrams, p. 290. A depiction of the orbit of cellular automata with the time going from top to bottom for each row.

\*In CAs, each cell is updated depending only on the values of its cells in its local neighborhood, there is no other communication between the cells other than this. Despite this locality they are capable of producing large scale patterns and structures.
(Note they are a form of distributed computation.)

### CA behaviors

CAs rules have numbers. Rule 32 results in all white cells quickly, while rule 46 develops into a repeated striped pattern. P. 291.

Rules 150 and 110 result in aperiodic chaotic patterns (but have interesting recurring shapes produced). P.292-3
\*Rule 110 in particular is interesting in that it is aperiodic and chaotic in a way, but has patterns that are not simple chaos. Papers have been written on this rule that might be interesting to read.

Behaviors can be analogous to the behavior of repeated functions.

#### Behavior classifications (S. Wolfram)

- space time diagram quickly turns all white or all black. Goes into simple fixed point. (Rule 32)
- configuration freezes into regular periodic pattern (rule 46)
- produces chaotic patterns, aperiodic, remembers their random initial condition, process of iteration does not produce significant order.
- \*complex: combinations of both chaos and order, aperiodic, but more structured patterns than the chaotic rules. (Rule 110)

These are four coarse descriptions of classes of behavior introduced by Steve Wolfram.

Orbits of CAs are bounded: there are only two possibilities for each cell and no infinity for them to move toward.
Note that technically it is impossible for CA to be truly aperiodic since there are a finite number of possible configurations, it will eventually return to a previous configuration.

\*Good essay on patterns and complexity: "Is anything ever new? Considering emergence" by James Crutchfield.

\*A common property of dynamical systems is that it's difficult to tell if the orbit will be chaotic etc. from looking at the rule. The only way to know is to start iterating and observe the behavior.
(Note that if the initial condition is a single black cell,or a single cell seed, in the case of CA, we can predict simpler and more ordered orbit patterns will occur (not exactly what, but only that it will tend to be ordered and simpler) - p. 296-7, see rule 46 applied to an initial condition of a single black cell - it produces the stripe, but only a single stripe as expected).

Key aspect of the patterns produced by CA (another example is rule 22 producing a sierpinski triangle) is that they have no hindsight - they produce the next row based only on information in the current row yet produce global patterns. This is on contrast to starting with a complete shape and removing our bending portions of it to produce a fractal.
This is another way fractals can be produced (rule 22) and confirms collage theorem that any fractal can be produced by a random process.

### types of CAs

- Elementary cellular automata: only two states (black or white), 3 cell neighborhoods, one dimensional configurations.
- Two dimensional CAs: the neighborhoods are grids and determine the next two dimensional grid of cells as output. These are used more for modeling physics and biology phenomenon.

Number of states (denoted by K) - can increase to more than just two states (i.e. K=4 could be four different colors)

Cell Radius (denoted by r): how far to the right or left a neighborhood extends. For an ECA, the radius is one since a neighborhood is 3 cells (one to the right and one to the left of center cell).
Possibilities: a radius-1 CA has 2^3 neighborhoods (8 - 2 states raised to power of neighborhood cell size, 3), and 256 possible combinations (2^8 - 2 states raised to the num of neighborhoods).

Increasing to just a radius-2 CA (5 cell neighborhood),you can have 4294967296 possible configurations! P. 300-1

Good book: Melanie Mitchell's Complexity: A guided tour.

## Differential Equations (ch. 28)

- dynamical systems that describe continuous change, where the variable of interest has a value at every instant. P. 303
- used to model phenomena that change continuously (I e. By measuring a rate of change over a time interval or instant -- a infinitesimally small time interval)

Example of continuous iteration: temperature of cup of coffee as it cools. There is no discrete step between iterations. The temperature could be thought of as the output of a function that accepts the current instant in time as input: T(t)

f(t) - something is a function of time.

\*\*Best book on intro to calculus: Calculus Made Easy (1998) by Silvanus Thompson.
Ch 11 has good overview in computational beauty of nature (flake)

### Measuring instantaneous rates of change

- to measure a rate you need two instances of measurements: you divide the rate of change by the time between the instances, but an instant moment would have a time value of zero (cannot divide by zero).
  The way around this is to start measuring smaller time intervals, i.e. get t=1 and t=1.1 rate, then calculate t=1 and t=1.01 and continuously get closer to zero time between the two timesteps.
  P. 305

An instantaneous rate of change is the rate of change at an exact 't' time.i.e. coffee is cooling at -12 deg per minute at t(1).

### Notation:

- h(t) -- height of a tree at time t
- dh/dt -- "the instantaneous rate of change", small delta (infinitesimally small value) of change height over change in time
  - this is the Derivative of h(t), a.k.a. the instantaneous rate of change
- h'(t) = dh/dt -- alternative notation for derivative: "h prime of t"

### Euler's method (p. 310)

- see good summary on p. 325
  -aoproximating instantaneous rate of change as above.
- not efficient compute wise and other methods are often used in practice that are less expensive computationally.

df/dt = F(f(t))

Df/dt is a symbol that means change per step (I.e.
2 degrees per minute). NOTE: this represents the value and is not meant to be read letter by letter or as a fraction or equation. it is a symbol in entirety representing the value for the rate of change.
f(t) is some function (I e. The temperature of a coming cup of coffee).
F(f(t)) is some expression involving the function.

Ex: f(t) - 613 + (f(t))^2

The task in a differential equation is to find and solve for the unknown function f(t) - to find the function itself

- you need the initial condition, the value of f(t) when t = 0. (I.e. the temperature of the coffee when starting to measure)
- you also need to choose the step size dt(delta of time)

### other solutions

Note: even calculus cannot always solve exactly, so approximations for solutions to differential equations are necessary - generally you can carry them out on a computer.

- calculus is needed to solve accurately (Euler's method is easier to understand and only approximate)
- another more efficient method is Runge-Kutta methods

### the logistics equation as a differential

- has a fixed point of the capacity, ex. 100
- can solve for a fixed point (various initial conditions wind up at a value (the rate of change becomes zero:
  df/dt = 0
  I.e. when the rate of change of the population at time t is zero)

P. 314
Logistic equation rate of change differential:
dP/dt = rP(1 - P/K)
Where K = pop capacity and r is the growth rate.

Note that in the logistic differential equation, the rate of change depends entirely on P, the current value of the population, in other words, it is a function of P:
dP/dt = F(P)

So with K = 100, we get a fixed point at 100:
4P(1 - P/100) = 0 // P = 100 yields zero

### Plotting differential equations

3 ways:
P. 316-317 for examples

- plot F(f(t)), x axis is the input of the dependent variable (P for example in the logistic differential equation), and y axis is the results of the output of the equation. Fixed points are on the zero y axis intersections.
- time series: the time steps are on the x axis and the initial condition input of the function on the y axis. Fixed points are shown as straight lines. Can visualize attracting n repelling fixed points
- phase line: horizontal line with arrows moving towards or away from fixed points represented as dots on the line (fixed points being where the rate of change on the y axis of first graph is zero)

Todo: come back to this and plot these for differential in this section using Python matplotlib etc

### one dimensional differential equations

dX/dt = F(X)

- this is one form of a one dimensional differential, called an autonomous equation since the growth rate is independent of time (it only depends on the input X and is independent explicitly of time).

- the logistic differential equation is an example

- unlike with one dimensional iterated functions, one dimensional differential equations are not capable of producing periodic and chaos behavior.
  Because their solutions are continuous, they can never change direction so chaotic and periodic solutions are not possible.
- The solution can only increase or decrease on a time series graph (or be constant if the initial condition is a fixed point).
  It cannot both increase and decrease, see p.318.
  In a time series graph the curve cannot visit the same X value (input) multiple times.

Important exception, p. 318:
One dimensional equations that have more than one dependent variable can have periodic and chaotic behavior.
dX/dt = 10X(t) + sin(2t)
Here there is dependence on both X and t, so the solution can both increase and decrease

\*\*To iterate on differentials or rates of change, you need an initial condition. You plug that in to get the first rate of change and then multiple that condition by the rate of change to get inputs for the next value.

## 2 dimensional differential equations

- p. 321
- the Lotka-Volterra (LV) equations, model of two interacting populations.

Example of rabbit and fox populations with lv model on p. 323.
It is two dimensional in the sense that there are two unknown functions: rabbit population R and fox population F.

- example of a coupled system of differential equations: the growth rate of rabbit population is dependent on the fox and rabbit population and vice versa. The populations are not independent of each other.

To iterate (using Euler's method) and plot, you just need an initial condition and a time step value.
Ex: dX/y = y, dy/dt = -0.5X - 0.4Y
Start with x(0)=5 and y(0)=6 (X(t) and Y(t) go on y axis) and a step size of 0.0001 (ticks on the x axis)

Most differential equations in two dimensions are solved using Euler's method or something similar (to get approximate solution as opposed to using calculus for an exact solution).

2 dimensional differential systems can be plotted on separate time plots or on phase planes (no time reference) or phase spaces (a geometric representation of state variables, i.e. tge popultaion of rabbits and foxes) P. 326-7

State variable: the state determines the future, knowing the value at one point in time determines uniquely determines it's value at all points in the future.
Ex: minute hand of clock: state var could be the number of hours or an angle between 0 and 360 degrees.

Phase portrait: diagram showing a global view of solutions to the differential equation to understand the behavior of the solutions at all initial conditions. Ex on p. 328

Neutral stability: cycles or periods that are neither attracting or repelling.
Example, the LV systems global behavior consists of oscillations of neutral stability. P. 328-9

### Attractors in 3 dimensional systems

Example of attracting point on p. 329 (oscillates, narrows and tends to zero where it sticks over time.)
The phase plot looks like a swirl or black hole or spiraling galaxy shape.

- atractractors can be oscillations, cycles too. Multiple initial conditions can gravitate towards and end up in an oscillating cycle.
  Example: the Van Der Pol equation, p. 330).

- this is called a limit cycle: a closed attractor in a two dimensional phase plane (a recurring periodic shape of multiple init conditions lead to without plotting the path there, see. P. 331 the van der pol attractor)

For the type of two dimensional equations above in the form of dX/y = f(X,Y) dY/dt = g(X,Y), tge only two types of global stable behavior is a fixed point or a limit cycle.

\*\*Phase trajectories cannot cross over each other because the rates of change of x and y are uniquely determined for each x,y point in phase space. See p. 332.
Because of this aperiodic and bounded behavior cannot occur, so two-dimensional continuous deterministic dynamical systems CANNOT be chaotic! (You need more dimensions).

## Strange Attractors

- In chaotic differential equations such as the Lorenz or Roessler equations (3 dimensions)
- with three dimensions, chaotic behavior is possible (unlike with two dimensional differential equations).

### the Lorenz equations

- simplified equations used to measure weather dynamics
  There are 3:
  dX/dt = \sigma(y - x)
  dy/dt = x(\rho - z) - y
  dz/dt = xy - \beta(z)

The Greek letters are parameters.

Given initial conditions of sigma = 10, rho = 8 and beta = 2.667, an attractor or fixed point occurs at x = 4.25, y = 4.25 and z = 7. Other initial conditions also are pulled to this point/attractor.

Note that in 3 dimensional phase space it's possible for a closed trajectory to curve above or below itself so complicated knotted and twisted structures are possible (unlike with 2 dimensional trajectories like can der Pol eq. That can't cross itself).

### a chaotic differential equation (3 dimensions, Lorenz equations)

When sigma = 10, rho = 28, beta = 2.667, this produces aperiodic (chaotic) behavior in the long term trajectory. The phase plane plot shows two circles weaving left and right around each other, see p. 341.
The pattern it visits the left and right lobe circles is:
RLRRRRLLLRLLRRRLRLLLRLLRRR
(Might be interesting to plot out stickings of other orbits like this or continue aperiodic mapping)
If x=30,y=30,z=30 initial condition is compared with trajectory of x=30.01,y=30,z=30 then SDIC is shown as they differ by t=9.

## Strange Attractors

- complex, stable attractors in phase space on which the dynamics are chaotic

### The Lorenz Attractor

- a strange attractor: these are attractors with a mixture of order and disorder. Almost all initial conditions get pulled towards a stable shape in phase space, but the behavior of individual trajectories are unstable and sensitive to initial conditions. Two orbits close to each other can soon diverge, but still follow a similar shape.
  P. 343-344

There is an order to strange attractors in that one can be certain the orbit will remain close or on the attractor, but the exact path as it leaves across the attractor is unpredictable.

3 characteristics of strange attractors:

- multiple initial conditions pulled towards it
- trajectories can be Chaotic/aperiodic and sensitive to initial conditions
- it's fractal (dimension of Lorenz Attractor is approx. 2.05)

\*\*Strange Attractors are a generic property and common phenomenon seen in 3+ dimensional differential equations.

### the Roessler Attractor

- another strange attractor based on Lorenz equations (simplified) that is easier to visualize than the Lorenz Attractor. P. 345
  Visual on p. 347
- As with Lorenz Attractor, trajectories on the attractor are chaotic, while orbits off the attractor are quickly pulled to it.

dX/dt = -y-z
dy/dt = x + ay
dz/dt = b + z(x - c)

The behavior of orbit in the Roessler Attractor is to flow counter clockwise in a circle on the xy plane and rise and fall on the z plane at the back of the circle.
The orbits rejoin the main circle merging into the inner portion of the circle and fold over on themselves.
It's a system that stretches and folds over itself repeatedly. There is a small space between the sheets as they continually fold over themselves.
See p. 348 for visual

The spacing of the folded over layers of orbits on top of each other has the structure of a cantor set (see p. 347)

It's not surprising that chaotic phase space plots have stretching and folding:
Stretching is responsible for the butterfly effect pushing nearby trajectories apart.
Folding us necessary to keep orbits bounded so they don't tend towards infinity.

Just like the logistic equation (1 dimensional) which captures stretching and folding if higher dimensions, same characteristics and universal period doubling behavior can also be seen in the Roessler Attractor vividly.

It's possible to approximate higher dimensional systems with lower dimensional ones (Henon equations used to approximate cross section of Lorenz equations).
You can use 1 dimensional equations to capture one part of motion for a 3 dimensional system.

#### First recurrence or Poincare map

- Deriving lower dimensional systems from higher dimensional ones by making a two dimensional slice through a high dimensional space.
- the slice made is a Poincare section.

Recommended books on strange attractors:

- Strange Attractors chapter in Gleick
- chapter 6 Stewart
- more technical: ch. 12 of Peitgen, Jurgens, Saupe
- clear math explanation of Lorenz equations: ch. 9 in Strogatz

### Attractor reconstruction

- given one dimensional data, can recontruct the attractor in higher dimensions.
- also known as the theory of embedding.
- decent overview in ch. 5 of Ott,Sauer,York
  Recs on p. 349

### Takeaways conclusion

- \*\*\*\*SDIC makes chaotic orbits unpredictable, but a histogram formed from a chaotic orbit takes on a predictable shape, enabling predicting the long term average behavior of a chaotic orbit
  Chaotic behavior can be statistically stable.
  (Review histogram of chaotic orbit chapters)

  - Ex: logistic equation with r= 4 is chaotic, but making a histogram of that and a close orbit itineraries that differ greatly in trajectory will in the long term be identical shape (the histogram shapes)

- one of the main ideas in chaos is that the systems are SO deterministic that a small imprecision in our knowledge of initial conditions are quickly amplified foiling any predictions.
  (I.e. starting at a point very close to another start but getting different trajectory).

- randomness can be caused by order and vice versa(chaos game producing sierpinski triangle)

### Books and Further Reading

- See appendix C
- \*\*\* Gleick - Chaos: Making a New Science
  - Feldman's book was written as companion to this and highly recommended, not a dense read
- nature's patterns: a tapestry in three parts, Shapes Flows And Branches (3 books) by Philip Ball. Good book about patterns in nature

### Intrinsic Emergence

- from [paper on pattern emergence](https://csc.ucdavis.edu/~cmg/papers/EverNew.pdf)
- In the emergence of coordinated behavior, though, there is a closure in which the patterns that emerge are important within the system. That is, those patterns take on their “newness” with respect to other structures in the underlying system. Since there is no external referent for novelty or pattern, we can refer to this process as “intrinsic” emergence.
- The system itself capitalizes on patterns that occur within it and uses them.
- Example of randomness leading to order: self excluding random walk - a starting point is moved from in a random direction, the process repeats, but the random direction cannot be the previous direction moved. This produces fractal structures. (also the chaos game)
- a balance between order and randomness can be reached and used to define
  a “best” model for a given data set. The balance is given by minimizing the model’s size
  while minimizing the amount of apparent randomness
- Physics does not have a way of detecting the boundary between order and randomness in natural structures (i.e. measuring phase transitions).
  - A key feature of natural structures is the two parts of order and disorder and the boundary between the two.
- An ideal model minimizes the data size (only as many causes as necessary) and minimizes error (noise)
- Computational theory: a machine can be reconstructed from a series of discrete
  measurements of a process
- There are machine classes with finite memory, those with infinite one-way
  stack memory, those with first-in first-out queue memory, and those with infinite random access
  memory, among others. When applied to the study of nature, these machine classes reveal
  important distinctions among natural processes. In particular, the computationally distinct classes
  correspond to different types of pattern or regularity
