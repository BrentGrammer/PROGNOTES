# S3 Simple Storage

### **PRIVATE BY DEFAULT** - permissions are locked down and you have to allow access and policies to use S3.

- Only the account root user has access to a bucket by default.

- inifitely scalable storage (Buckets can store unlimited amount of data)
- Public service (runs in public AWS Zone)
- Region based (possible to replicate across regions)
- Object storage system (not a file or block storage system)
  - Good for accessing the whole object (images, audio files, etc.)
  - Not block storage that you can mount as a drive into images etc.
  - Good for offloading data and storing it
  - usually used for input and output to AWS services (most services offer integration with S3)

## Use Cases

- backup and storage
- Disaster Recovery
- Archiving
- Hybrid Cloud Storage (extend from on premise)
- Application hosting
- Media hosting
- Data lakes and big data analytics
- Software delivery
- Static website hosting

### Objects: files stored in S3

- Two parts: a key and a value (key is like a file name, the value is the data)
- Have a key which is the full path: s3://my-bucket/myfile.txt
- key is composed of the prefix (directories with slashes) and the file name
- **MAX SIZE**: From 0 bytes to 5 TB
  - must use a multi-part upload to upload larger files
- Have Metadat and tags that can be referenced and a version ID
- Objects are not actual files - they are just objects with a key.

## Buckets:

- like directories for the files
- Must have **globally unique name** across all regions and all accounts
  - see [rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html?icmpid=docs_amazons3_console)
  - Must be between 3 and 63 chars
  - Must start with lowercase letter or number
    - CANNOT begin with `X` or `N`
  - can contain dots and dashes
  - bucket names can't be IP formatted i.e. 1.1.1.1
  - soft limit of 100 buckets in an account (not per region), hard limit of 1000 (using support requests) - remember this to structure a system with many users - use prefixes with one bucket instead of a bucket per user for example.
- Tied to a specific region! Data has a primary home region and does not leave that unless configured to.
- Has to be defined at the region level and assigned a region
- there is naming convention restrictions
- Blast Radius is region level. (large scale failures are confined to the region)
- Note: all objects are stored at the root level of a bucket (there is no file directory heirarchy)
  - you can still have keys that have prefixes in the key that can simulate nested folders

#### Security Issue: Bucket names

- You are charged for bad requests or denied requests so make sure your bucket name cannot be guessed and is sufficiently unique and hard to guess.

### Cleaning/Deleting up buckets

- two steps:
  - you need to empty the bucket (select the bucket on the s3 page and click `empty` in the console)
  - and then click `delete`.

## Permissions for S3 Buckets

### S3 Bucket Policy

- a type of **resource policy** (a policy that is attached to a resource instead of an identity)
  - Controls who can access that resource
  - Unlike identity policies, resource policies can allow/deny identities from the same account or a different account. \*Ability to grant other identities from other accounts permissions\*\*
  - Can also allow Anonymous Principals (grants access to the world)
- Resource policies have a `Principal` property that defines which identities/principals are affected by the policy (who is impacted)
  - Identity policies do not have this as the principal for them is the identity
- You can only have ONE bucket policy on a Bucket, but it can have multiple statements.

### When to use Identity vs. Resource Policy

- If you need to grant permissions to many different resources across an AWS account, then use Identity policies (not every resource has resource policies and you'd need to create a resource policy for every service - with an identity policy you can have one policy that specifies permissions for all the different resources)
- Identity management in one place means you want to use IAM (resource policies do not have a central place for management)
- Resource policies make sense for allowing anonymous outside users access to a service.

### Access

- For any identity in an AWS account is accessing a bucket inside that same account, then access is a combination of all the applicable identity policies for the user and the resource bucket policy.
- For anonymous access, only the bucket policy applies (no identity policies since they are not logged in)
- For users from another AWS Account,
  - they need an identity policy to allow access to your bucket,
  - and you need to allow them in the bucket policy (2 step process)

### Implementing Bucket Policies

- You can specify IP addresses that are allowed, for example:

```json
{
  "Version": "2012-10-17",
  "Id": "BlockUnLeet",
  "Statement": [
    {
      "Sid": "IPAllow",
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:*",
      "Resource": "arn:aws:s3:::secretproject/*", // copy the arn on the edit bucket policy page and make sure to leave the `/*` at the end after it to allow access to all objects
      "Condition": {
        "NotIpAddress": { "aws:SourceIp": "1.3.3.7/32" } // deny everyone except this IP (this statement does not apply and this IP gets any other permissions allowed.)
      }
    }
  ]
}
```

- Can also specify multi factor auth is required to use the bucket, etc.

## Pricing

- https://aws.amazon.com/s3/pricing/
- Per GB per month charge (very cheap)
- Transfer charge for data in and out per GB (very cheap)
- Charged for Operations such as GET, PUT, DELETE etc.
  - charged per 1000 requests
  - Be careful if you have a lot of users or operations - charges add up.

# S3 Security

- User Based
  - IAM Policies - which API calls are allowed for a specific IAM user
  - Use case - attach a IAM policy to a user to access a bucket (no bucket policy needed)
  - For EC2 Instances use a role that has access policy for S3 buckets attached to it
- Resource Based
  - Bucket Policies allow cross account access
  - Use case is Public Access - attach a bucket policy that allows public access
  - Use case is a user that is on another AWS account that needs access - use a bucket
    policy
  - can also use them to force encryption
- IAM principal can access an S3 Object if either the user IAM policy or bucket policy allows it.
  - \*There is no explicit DENY policy
- Encryption - use encryption keys for bucket contents

### ACLs (Access Control Lists)

- Less commonly used currently and are considered legacy vs. bucket policies
- not as flexible as bucket policies and can only have simple permissions
- Can be applied to subresources such as objects in buckets.
- ACLs cannot affect a group of objects which is a major drawback from bucket policies which allow that flexibility.

## Public Access

- When creating a bucket if you uncheck **Block all** access it does NOT make the bucket publically accessible, but will allow you to configure it for public access if desired. It is a fail safe that will override any configuration you make to make the bucket public.

  - Block Public Access policies were implemented for security and apply regardless of the policies created (they apply only to anyonymous principals)
  - If you have granted public access via a bucket policy and it still doesn't work, these settings are probably set and you need to adjust them to allow access.

- All objects and buckets are private by default (you need to authenticate first) unless configured otherwise.
  - Example if you open an object link from the console in a new tab you will get Access Denied because you're accessing it as an unauthenticated user in a new fresh tab.
  - You need to use the `Open` button in AWS S3 Console to open the file in the browser which will authenticate you.

### Setting up policy for public Access

- You need to uncheck all the block public access settings first
- Can use the policy generator in the GUI to generate a policy
- The ARN should end with a slash and the bucket directories or files
  - `<arn>/*`
- Useful for allowing accesses to websites hosted on S3

### Static Website Hosting

- Enable the static website option in properties for the S3 settings
- make sure block settings are disabled and policy for public access is setup.

# Object Versioning

- [video explanation](https://learn.cantrill.io/courses/1101194/lectures/25870664)
- [Video demo implementing versioning](https://learn.cantrill.io/courses/1101194/lectures/25871102)
- Can enable file versioning at the bucket level
- Any operations that modify an object with versioning enabled will generate a new object and leave the original in place.
- To see versions in AWS console select the `Show versions` toggle on the specific bucket page [in AWS console](https://learn.cantrill.io/courses/1101194/lectures/25871102) (see timetamp 6:12)

### Suspending Bucket Versioning

- **IMPORTANT**: Once you enable versioning, you cannot disable it and go back to non-versioned objects!
- Versioning can be suspended on a bucket as an alternative and re-enabled later if needed.
- AWS Console > S3 > Click bucket > Properties Tab > Versioning section - click Edit, select Suspend

### Bucket Ids

- With versioning off, the `id` attribute on a object in a bucket will be `null`
- Turning Versioning on will allocate an `id` value to each object
- Same key (filename) overwrite will update the version with a new `id` and preserve the original object with it's `id`

### Current Version

- The newest version of an object in a versioned enabled bucket is called the `Current Version`
  - If a version is not specified in the request, then the current version will always be returned

### Deleting versioned Objects (Delete Markers)

- [video demo of delete markers](https://learn.cantrill.io/courses/1101194/lectures/25871102) at timestamp 9:58
- IF no version is specified in delete request, S3 will create a new special version of the obj known as a "Delete Marker"

  - The object isn't actually deleted, but the delete marker will hide all previous versions of that object

  #### Delete Markers

- Deleting the "Delete Marker" will return the last current version of the object to be active again.
  - **Toggle off `show versions` on the bucket page, select the shown current version of the object and then click `Delete` to add a delete marker**
  - equivalent to an undelete for the object

#### Permanently Deleting Objects

- To Delete an object totally, you need to specify the version. The next most recent version if it exists will become the new current version.
- **Toggle on `show versions` and then select the version to delete, then click `Delete`**
- This makes the next most recent version of the object the current version.

### Storage considerations with versioning

- All objects remain and will incur storage costs and space
- The only way to zero out costs is to delete the bucket and re-upload all the files
  - Suspending does NOT cancel the storage costs.
- Best practice is to version buckets
  - restore from unintended delete and rollback file versions. To restore a file note that you delete the delete marker for it
- Note: files not versioned before enabling it will have a version of "null"
  - suspending versioning does not delete previous versions
- Setup: Enable bucket versioning in the properties tab of aws console gui for s3 bucket. This will auto generate versions of files uploaded for that bucket

## MFA Delete

- You can optionally enable this in the versioning configuration
- Multi Factor Auth will be required to change bucket versioning state
  - includes switching to suspended versioning, or deleting fully any versions of an object
- Requires a serial number plus code generated and pass that with API calls to delete or change version states.

# Access Logs

- for audit purposes or data analytics
- Logs are enabled and then written to another S3 bucket
  - NOTE: you need to create a new bucket for logging!
  - In the console you need to find menyu to enable Server access logging
    - For target bucket, remember to add `s3://` before the bucket name. You can also add a `/logs` to the end of the path
- Can take an hour or two before logs start populating after enabling

# S3 Replication

- CRR - Cross Region Replication
  - Use cases: compliance, lower latency access for distant/dispersed users, replication across accounts
- SRR - Same Region Replication
  - Use Cases: log aggregation, live replication between prod and test accounts
- Replication is Asynchronous in the background
- **Must enable (object/bucket) versioning in source and destination/target buckets**
- Bucket replicas can be in different accounts
- Must enable IAM permissions so buckets can copy over files
- Setup: Go to source bucket in AWS console and to Management tab, then create a replication Rule
  - note that version ids are replicated as well for files

# S3 Storage

- Types
  - Amazon S3 Standard - General Purpose
    - 99.99% Availability
    - Used for frequently accessed data
    - Low latency high throughput
    - can sustain 2 concurrent facility failures
    - Use Cases: Big Data Analytics, mobile and gaming apps, content distribution
  - Standard-Infrequent Access (IA)
  - One Zone-Infrequent Access
    - less frequently accessed files but requres rapid access when needed
    - lower cost than Standard
    - S3 Standard IA: 99.9% available (less than standard), used for disaster recovery and backups
    - S3 One Zone IA: 99.5% available, used to store backup copieds of data you can recreate or on prem data. \*Data is lost if an AZ is destroyed
  - Glacier: Low cost cold storage - pay for storage and retrieval cost
    - Glacier Instant Retrieval: millisecond retrieval - good for data accessed once a quarter. Min storage duration of 90 days, good for backup with fast retrieval
    - Glacier Flexible Retrieval
      - Expedited - 1 to 5 minutes retrieval
      - Standard - 3-5 hours for retrieval
      - Bulk - 5-12 hours for retrieval. This is free cost retrieval
      - Min. storage is 90 days
    - Glacier Deep Archive: long term storage
      - standard is 12 hours retrival, bulk is 48 hour wait for retreival
      - min. storage duration is 180 days
  - Intelligent Tiering
    - Small monthly monitoring and auto-tiering fees
    - No retrieval charges
    - Moves objects autopmatically between access tiers based on usage
    - Tiers:
      - Frequent Access: default tier
      - Infrequent Access: objects not accessed for 30 days
      - Archive Instant Access: not accessed for 90 days
      - Archive access(optional tier): configurable from 90-700 days
      - Deep Archive access(optional tier): configurable from 180-700 days
- Can move between classes manually or with lifecycles

S3 Durability: 11 9's - 99.999999999% - with 10,000,000 objects a loss can incur once every 10,000 years. This is for all storage classes in S3

Availability:

- Depends on storage class

- Can create Lifecycle Rules to determine when and where to move objects between storage class tiers based on your settings.

# LOCKS

- S3 Object Lock
  - WORM - write once read many - blocks object from editing or deletion for a time
- Glacier Vault Lock
  - Locks policy from editing for a time
- Used for compliance or scenarios where you need to retain a file unmodified for some time. Not even admins can edit it.

# S3 Encryption

- No Encryption
- Server Side - encrypted on backend after upload
- Client Side - encrypts before uploading

# AWS Storage Gateway

- Hybrid cloud solution
- Bridge on prem to S3 on the cloud
- Types: Tape Gateway, Amazon S3 File Gateway, Amazon FSx file Gateway, and Volume Gateway

# Performance and Reliability Optimizations

## Multipart Upload

- Uploading by default in S3 is a single stream of data: Data > (s3:PutObject) > Bucket
  - Note: Single uploads are limited to 5 GB (though practially, you would never rely on one stream for that size of data)
- If Uploading fails, the whole process has to be restarted from the beginning (any data that did make it is lost and you have to start the upload from the beginning)
- This limits the speed and reliability (if connections are unreliable)
- Even on fast connections, downloads/uploads usually should occur on multiple streams

### Multipart upload optimization

- \*\*Recommended to use this as soon as you can
- Breaks apart data into pieces to upload
- The minimum size required for using this is 100MB of data
- Uploads can be split into a max of 1000 parts
  - between sizes of 5MB to 5GB (the last part can be smaller than 5MB)

#### Main Advantage of Multipart Upload

- If pieces of data fail to transfer, they can be restarted individually rather than having to restart the entire data transfer from the beginning as you would in a single stream.
- The transfer rate (sum of parts) is faster using mutlipart upload by leveraging parallelism

## Accelerated Transfer

- [video](https://learn.cantrill.io/courses/1101194/lectures/25871120) at 9:00
- Data transferred is sent over the public internet via a path that we have no control over, and it can be inefficient sometimes.
- S3 Accelerated Transfer uses the AWS Global network of Edge locations instead of the public internet to send data through more efficient paths

### Enabling S3 Transfer Acceleration

- See [video](https://learn.cantrill.io/courses/1101194/lectures/25996479)
- [AWS Transfer Accerlation Tool](http://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html)
  - This tool shows you a comparison of upload speeds from where you are to each of the AWS regions with and without Acceleration Transfer on.
- Go to AWS Console > S3 > Click on bucket > Properties Tab > scroll down to find Transfer acceleration

  - Enabling it creates a new endpoint for the bucket that you need to use for the feature (ex: `bucketname.s3-accelerate.amazonaws.com`) - resolves to a edge location based on where you are in the world.

- **By default Accelerated Transfer is switched OFF in S3** - you need to enable it to take advantage of this feature.
- **RESTRICTIONS**
  - Bucket name cannot have any periods
  - Bucket name needs to be DNS compatible
- Note: the data goes through the public internet initially to get to the Edge, but they are really close geographically and positioned well.

### Main Advantage of Acceration Transfer:

- The advantages of using S3 Transfer Acceleration increase as the distance you're transferring data over increases.
- Especially good for transferring data over longer geographic distances
